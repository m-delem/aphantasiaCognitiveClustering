---
title: "Reproducible manuscript"
abstract: "Mental images are a ubiquitous phenomenon for many people. In recent 
  years, attention has focused on a condition defined by the absence of mental 
  images - aphantasia. Individuals with aphantasia are found to perform as well 
  as typical imagers in most areas. Interestingly, several studies have proposed 
  that individuals with aphantasia might have a more 'semantic and abstract' 
  mode of functioning. The present study aims to better understand the cognitive 
  profile of individuals with aphantasia, by examining their performance 
  regarding semantic and/or abstract processing. To that end, 45 participants 
  with aphantasia and 51 controls completed questionnaires and behavioural tasks 
  assessing sensory and spatial imagery, verbal strategies, verbal and 
  non-verbal reasoning, and verbal and spatial working memory. Initial 
  comparisons suggested very few differences between individuals with aphantasia 
  and controls. However, an unsupervised clustering algorithm revealed three 
  clusters focusing respectively on visual imagery, spatial imagery and verbal 
  strategies, and two very distinct profiles of individuals with aphantasia 
  among these clusters. The first profile had low visual imagery but maintained 
  multisensory imagery, and had higher spatial imagery; the other had low 
  imagery across all sensory modalities, and focused on verbal processing. This 
  study shows that individuals with aphantasia should not be systematically 
  classified based on visual imagery only, but characterised according to 
  various aspects of their cognitive profile. This multifaceted approach could 
  provide a balanced view of the benefits and drawbacks of mental images and 
  help us to understand the mechanisms underlying the spectrum of individual 
  differences in representational formats."
vignette: >
  %\VignetteIndexEntry{Reproducible manuscript}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
toc: true
echo: false
number-sections: true
crossref:
  chapters: false
lightbox: true
title-block-banner: true
bibliography: "references.bib" 
csl: "apa.csl"
---

# Beyond visual imagery: Unsupervised clustering reveals spatial and verbal cognitive profiles in aphantasia and typical imagery {.unnumbered}

**Keywords**: aphantasia, mental imagery, individual differences, cognitive
profiles, reasoning, working memory, unsupervised clustering



# Introduction

Visual imagery, commonly referred to as "seeing with the mind's eye", designates
the pseudo-perceptual visual experience of mental images in the absence of the
corresponding external stimulus [@pearsonHumanImaginationCognitive2019]. There
are large individual differences in visual imagery vividness (i.e. the intensity
and detail of mental images) across a spectrum going from the absence of mental
imagery, a phenomenon recently named "aphantasia"
[@zemanLivesImageryCongenital2015], to extremely vivid and perception-like
imagery, named "hyperphantasia" [@zemanPhantasiaPsychologicalSignificance2020].
The introduction of the term "aphantasia" in 2015 led to a wave of research on
the subject, exploring its underlying causes and consequences and potential
positive or negative outcomes. A large body of research on aphantasia mainly
identified potential deficits associated with it. Specifically, the condition
has been associated with a reduction in autobiographical memory
[@dawesCognitiveProfileMultisensory2020; @dawesMemoriesBlindMind2022;
@miltonBehavioralNeuralSignatures2021;
@monzelHippocampaloccipitalConnectivityReflects2023], lack of temporal and
future imagination [@miltonBehavioralNeuralSignatures2021], increased
prosopagnosia [@miltonBehavioralNeuralSignatures2021;
@palermoCongenitalLackExtraordinary2022;
@zemanPhantasiaPsychologicalSignificance2020], reduced dreams
[@dawesCognitiveProfileMultisensory2020], or decreased motor simulation
[@dupontExplicitImplicitMotor2024]. This focus on deficits has left the
potential positive aspects of aphantasia largely unexplored, even though
empirical evidence from recent studies has shown that individuals with
aphantasia performed as well as those with "typical" visual imagery in various
types of tasks presumed to require this ability, such as visual or visuo-spatial
working memory [@keoghVisualWorkingMemory2021;
@pounderOnlyMinimalDifferences2022; @reederNonvisualSpatialStrategies2024].
Research that hinted at advantages of aphantasia have focused mainly on
emotional processing. Individuals with aphantasia have been shown to be less
prone to sensory sensitivity [@danceWhatLinkMental2021], less reactive to the
reading of frightening scenarios [@wickenCriticalRoleMental2021], and less
sensible to intrusive memories [@keoghFewerIntrusiveMemories2023] suggesting
that aphantasia could help to reduce sensory overwhelm, and potentially protect
against emotional overreaction.

Recently, Monzel et al. [-@monzelAphantasiaFrameworkNeurodivergence2023]
proposed that aphantasia should be understood within the framework of
"neurodivergence" as a state representing atypical but functional cognitive
processing, with advantages and disadvantages. The specifics of this
"alternative thinking" and its advantages, however, remain to be understood.
Interestingly, @zemanPhantasiaPsychologicalSignificance2020 found that
individuals with aphantasia seemed more likely to work in STEM fields (Science,
Technology, Engineering, and Mathematics), whereas hyperphantasics, at the other
end of the spectrum, were more likely to work in art-related professions.
Drawing on the patterns emerging from their large-scale exploratory survey,
@zemanPhantasiaPsychologicalSignificance2020 proposed a broad hypothesis that,
whereas hyperphantasia might be characterized by an episodic and sensorily-rich
mode of thinking, aphantasia might be characterized by a more semantic and
fact-oriented approach. The results and conclusions of
@zemanPhantasiaPsychologicalSignificance2020 are very similar to the
Object-Spatial-Verbal model of cognitive styles developed by
@blazhenkovaNewObjectspatialverbalCognitive2009 and its associated questionnaire
(Object-Spatial Imagery and Verbal Questionnaire, OSIVQ). Based on behavioural
and neuroimaging studies on healthy individuals
[@kosslynCognitiveNeuroscienceMental1995; @wallaceImageryVividnessHypnotic1990;
@kozhevnikovRevisingVisualizerVerbalizerDimension2002] and neuropsychological
studies of brain-damaged patients [@farahCaseStudyMental1988;
@bartolomeoRelationshipVisualPerception2002],
@blazhenkovaObjectSpatialImagery2006 showed that visual-object imagery (imagery
for colors, shapes, brightness, etc.) could be dissociated from spatial imagery
(imagery for location, movement and orientation).
@blazhenkovaNewObjectspatialverbalCognitive2009 challenged the prevailing
Visualizer-Verbalizer model of cognitive styles
[@paivioImageryAbilityVisual1971; @richardsonMeaningMeasurementMemory1977] to
introduce the spatial dimension as a major form of imagery and cognitive style
in its own right, alongside visual and verbal styles. They showed that several
widely used paradigms, such as the Mental Rotation Task
[@shepardMentalRotationThreeDimensional1971] or Paper Folding Test
[@ekstromKitFactorreferencedCognitive1976], often considered visual, were not
associated with visual imagery or visual cognitive styles *per se*, but with
spatial imagery and spatial cognitive styles
[@blazhenkovaObjectSpatialImagery2006;
@blazhenkovaNewObjectspatialverbalCognitive2009;
@kozhevnikovTradeoffObjectSpatial2010;
@vannucciIndividualDifferencesVisuospatial2006]. Consistent with the observation
of @zemanPhantasiaPsychologicalSignificance2020 of a prevalence of STEM
occupations in aphantasia and artists in hyperphantasia, several studies on the
Object-Spatial-Verbal model have shown visual-object cognitive styles to be
particularly prevalent among visual artists, while spatial styles are
over-represented in scientific fields and verbal styles prevail in literature
and the humanities, both among students and professionals
[@blazhenkovaObjectSpatialImagery2006;
@blazhenkovaNewObjectspatialverbalCognitive2009;
@blazhenkovaVisualobjectAbilityNew2010;
@kozhevnikovSpatialObjectVisualizers2005;
@kozhevnikovTradeoffObjectSpatial2010]. These results are corroborated by
various studies showing that spatial imagery is preserved or enhanced in
aphantasia, both on the subjective spatial scale of the Object-Spatial Imagery
Questionnaire [OSIQ, the first version of the OSIVQ without the verbal scale,
@blazhenkovaObjectSpatialImagery2006] and on various spatial rotation,
manipulation or spatial working memory tasks [@zemanLivesImageryCongenital2015;
@bainbridgeQuantifyingAphantasiaDrawing2021;
@dawesCognitiveProfileMultisensory2020; @keoghBlindMindNo2018;
@keoghVisualWorkingMemory2021; @pounderOnlyMinimalDifferences2022;
@reederNonvisualSpatialStrategies2024].

Several studies have also revealed a wide range of spatial,
sensorimotor/kinaesthetic, verbal or amodal memory strategies reported by
individuals with aphantasia in (supposedly) visual tasks
[@keoghVisualWorkingMemory2021; @reederNonvisualSpatialStrategies2024;
@zemanPhantasiaPsychologicalSignificance2020]. The diversity in modes of
thinking could therefore be distributed across several dimensions, including
visual-object or spatial representation, but potentially extending to verbal and
semantic domains. The verbal (or "propositional") aspect of representations and
cognitive strategies, although often mentioned as a potential candidate for
alternative strategies in visual aphantasia, has scarcely been studied. The
Object-Spatial-Verbal model of cognitive styles could allow to study verbal
representations in a coherent framework alongside visual and spatial imagery and
shed light on the cognitive strategies of individuals with aphantasia.
Therefore, the objective of the present study was two-fold: (a) to explore the
cognitive profiles of individuals with aphantasia using the
Object-Spatial-Verbal model of imagery and cognitive styles theorised by
@blazhenkovaNewObjectspatialverbalCognitive2009, and (b) to explore whether the
profiles might be related to cognitive performance.

We hypothesised that individuals with aphantasia would tend to adopt spatial and
verbal cognitive profiles, and that these profiles would be associated with
specific performance patterns. First, we hypothesised that the profiles might be
related to reasoning performance. Spatial imagery is known to be involved and to
play a major role in abstract reasoning [@waiSpatialAbilitySTEM2009]. In this
context, the absence of visual imagery in aphantasia and the priority and focus
on spatial representations in aphantasia
[@bainbridgeQuantifyingAphantasiaDrawing2021; @keoghVisualWorkingMemory2021;
@reederNonvisualSpatialStrategies2024] could be hypothesised to facilitate
abstract reasoning. Second, we hypothesised that spatial or verbal cognitive
profiles could explain individual differences in working memory performance,
depending on the modality involved. Previous studies failed to find differences
in working memory performance between individuals with aphantasia and controls
[e.g., @keoghVisualWorkingMemory2021; @pounderOnlyMinimalDifferences2022;
@reederNonvisualSpatialStrategies2024], but only took into account variations on
the visual-object dimension of imagery. Accounting for the use of spatial and
verbal representations in working memory could provide insight into the
processes and strategies underlying the performance of individuals with
aphantasia in various tasks [@pearsonRedefiningVisualWorking2019]. Third, we put
forward the very general hypothesis that if individuals with aphantasia have
distinct verbal cognitive profiles, they should have very good reading
comprehension skills. However, some studies have established a positive
correlation between reading comprehension and visual mental imagery [e.g.,
@suggateMentalImagerySkill2022], suggesting a central role for the latter. To
date no specific study of reading comprehension in an ecological context in
aphantasia could provide a definitive answer as to the advantages or
disadvantages of aphantasia in complex reading tasks involving verbal skills,
working memory and mental imagery. Finally, we hypothesised that the performance
of individuals with aphantasia in tasks supposed to require visual imagery might
be linked to a greater flexibility in switching to alternative strategies (e.g.
propositional, motor, etc.). In this case, they should be characterised by
particularly efficient executive functioning. Thus, the present study also
included a task designed to probe executive functions.

In sum, the present study aimed to gain a better understanding of the cognitive
profiles of individuals with aphantasia and their strategies for representing
and processing information. We sought to identify patterns of performance in
accomplishing various cognitive tasks from individuals with aphantasia and
controls and relate these to their subjective preferences for visual, spatial or
verbal processing. To this end, an online study was designed, integrating
questionnaires and behavioural tasks to assess visual imagery, spatial imagery,
verbal strategies, spatial, verbal and non-verbal reasoning, verbal and
visuo-spatial working memory, reading comprehension, and executive functions.
Based on previous work showing very few differences in cognitive performance
between individuals with aphantasia and controls [@keoghVisualWorkingMemory2021;
@pounderOnlyMinimalDifferences2022; @knightMemoryImageryNo2022], we expected
that dividing the participants into two groups according to visual imagery
ability would not fully explain substantial differences in task performance.
Thus, we planned to explore the hypothesis of hidden sub-groups within the
sample using a data-driven unsupervised clustering method. This analysis
included all measures of cognitive abilities to assess similarities and
differences between participants. The proposed data analysis plan resulted in
clusters characterized by their visual, spatial, and verbal cognitive styles,
which were able to explain task performance. In the light of these patterns, we
then discuss the potential of the Object-Spatial-Verbal model for understanding
cognitive processes and strategies in aphantasia.

# Methods

No part of the study procedures or analysis plan was preregistered prior to the
research being undertaken. We report all data exclusions, all
inclusion/exclusion criteria, all manipulations, and all measures in the study.

## Participants

Participants had to be French speakers, had normal or corrected vision and none
of the participants reported to have any known reading disorders. They were
recruited online both on groups unrelated to mental imagery (social networks,
French cognitive science information network, etc.) and on groups dedicated to
aphantasia and visual imagery. The study link was sent to participants who
volunteered by contacting the team by email. All procedures performed in these
experiments were in accordance with the ethical standards of the institutional
research committee and with the Helsinki declaration and its later amendments or
comparable ethical standards. Informed consent was obtained from all individual
participants included in the study . Participation was without compensation. As
the study was exploratory, the sample size was not determined *a priori*. Only
data from participants who completed all the tasks were included in the
analyses. Of the 1200 people who opened the link to the study, 96 completed all
the tasks, making up the final sample. The questionnaire statistics are detailed
in the results part below.

## Materials

### Questionnaires

The Vividness of Visual Imagery Questionnaire [VVIQ,
@marksVisualImageryDifferences1973] was used to assess visual imagery ability.
The VVIQ is a 16-item self-report scale that asks participants to imagine a
person and several scenes and to rate the vividness of these mental images using
a 5-point scale ranging from 1 ("*No imagery at all, you just know you're
thinking about the object*") to 5 ("*Perfectly clear and as vivid as normal
vision*"). Scores range from 16 to 80. The total score of 32, conventionally
used as a threshold to define aphantasia, is equivalent to a score of 2 ("*vague
and faint*") for each item in the questionnaire. The internal reliability
(Cronbach’s $\alpha$) of the VVIQ is .88 [@mckelvieVVIQPsychometricTest1995].

The Object-Spatial Imagery and Verbal Questionnaire [OSIVQ,
@blazhenkovaNewObjectspatialverbalCognitive2009] was used to evaluate imagery
strategies and cognitive styles. The OSIVQ is a 45-item scale that asks
participants to indicate the extent to which each of the statements applied to
them, about visual-object imagery ability (e.g., "*When I imagine a friend's
face, I have a perfectly clear and bright image*"), visuo-spatial imagery
ability (e.g., "*My images tend to be schematic representations of things and
events rather than detailed images*") or verbal strategies for processing
information (e.g., "*When I remember a scene, I use verbal descriptions rather
than mental images*"), on a 5-point scale ranging from 1 ("*Totally disagree*")
to 5 ("*Totally agree*"). Each sub-scale (object, spatial, verbal) comprises 15
items whose values are added together to obtain a score ranging from 15 to 75.
Cronbach's $\alpha$ of the object, spatial and verbal scales are .83, .79 and
.74 respectively [@blazhenkovaNewObjectspatialverbalCognitive2009].

As mental imagery is a multi-sensory experience that is not limited to vision,
the Plymouth Sensory Imagery Questionnaire [Psi-Q,
@andradeAssessingVividnessMental2014] was used to assess imagery vividness
across various sensory modalities. The Psi-Q (in its short form) comprises seven
sets of three items for each of the following modalities: *Vision*, *Hearing*,
*Smell*, *Taste*, *Touch*, *Bodily Sensation* and *Emotional Feeling*. Each set
has a heading such as "*Imagine the appearance of...*"} and then three items.
Participants were asked to rate their image on an 11-point scale anchored by 0
("*No image at all*") and 10 ("*As vivid as real life*"), thus yielding scores
ranging from 0 to 33 for each modality. Cronbach's $\alpha$ of the 21-item Psi-Q
is .91 [@andradeAssessingVividnessMental2014].

### Tasks

The Raven Standard Progressive Matrices [hereinafter called Raven matrices,
@ravenRavenProgressiveMatrices1938] is a widely-used assessment to estimate
fluid intelligence (non-verbal visual perception ability) and abstract reasoning
(analogical and deductive reasoning abilities). The Raven matrices contains 60
items divided into five sets. Each question consists in completing a missing
figure in a matrix of $3 \times 3$ figures by extracting and following the
logical rules underlying the organisation of the matrices. Items are of
increasing difficulty. At the end of a set, the difficulty decreases again but
the logical rule changes, and the successive sets have increasing difficulty. A
shortened clinical version with two short forms of 9 items developed by
@bilkerDevelopmentAbbreviatedNineItem2012 was used, predicting the 60-item score
with good accuracy. Each of the short forms had correlations of $r = .98$ with
the long form, and respective Cronbach’s $\alpha$ of .80 and .83. This shortened
version represents a 70% reduction in the number of items to be administered and
in test-taking time, for psychometric characteristics similar to those of the
full form [@bilkerDevelopmentAbbreviatedNineItem2012].

The Spatial Reasoning Instrument [SRI, @ramfulMeasurementSpatialAbility2017] is
a 30-item test for measuring spatial ability along three constructs: mental
rotation, spatial orientation, and spatial visualisation. The test showed good
validity an psychometric properties in three areas: (a) the exploratory factor
analysis of the subscales (mental rotation, spatial orientation, spatial
visualisation), (b) the Rasch analysis of item reliability within each
construct, and (c) the significant correlations ($r \in [.33, .62]$) and person
separation reliability with four existing well-regarded instruments measuring
spatial reasoning, the Card Rotation Test, the Cube Comparison Test, the Paper
Folding Test \[all from @ekstromKitFactorreferencedCognitive1976\] and the
Perspective Taking (Spatial Orientation) Test
[@kozhevnikovDissociationObjectManipulation2001]. The internal reliability
(Cronbach’s $\alpha$) of the SRI is .85.

The Similarities sub-test of the Weschler Adult Intelligence Scale [WAIS-IV,
@wechslerWechslerAdultIntelligence2008] is a well-known assessment to estimate
verbal comprehension abilities. Specifically, this test assesses both verbal
concept formation and verbal abstract reasoning. It comprises 18 pairs of words
in which the participant has to identify the underlying qualitative relationship
(e.g., "*How are DREAM and REALITY similar?*"). Accurate answers (rated
according to a standardized response scale) receive two points, approximate
answers one point, and vague or incorrect answers zero, giving a maximum score
of 36. After three zero scores, the task stops. Due to the internet-based nature
of the experiment, all participants passed all the items, but only the *scoring*
of their answers stopped after three incorrect answers. The scoring was carried
out manually, using double scoring by the first two authors of this article,
blind to the groups and participants. All participants performed above the fifth
percentile on the Similarities WAIS-IV sub-test (score $\geq$ 12/36), thereby
confirming that none of the participants presented a deficit in semantic oral
language skills.

Reverse Corsi blocks is a spatial memory span task [@gibeauCorsiBlocksTask2021]
assessing visuo-spatial working memory. The task consists in presenting the
participant with a grid of blocks in a frame, then displaying a sequence of
blocks (the blocks changing colours in turn), at a rate of one per second, and
asking the participant to recall it in reverse order, from the last block to the
first. The task began with a short sequence of three blocks, increasing with
each success or decreasing after two failures, over a fixed total of 14 trials.
The average number of blocks recalled correctly at the correct position was
retained as the score for the task.

The reverse digit span is a verbal memory span task assessing verbal working
memory [@blackburnRevisedAdministrationScoring1957]. The task involves
presenting numbers at a rate of 1 per second, and then asking the participant to
recall them from the last to the first. The task begins with a short sequence of
three digits, then lengthens with each success or decreases after two failures,
over a total of 14 trials. The average number of digits recalled correctly at
the correct position was retained as the score for the task.

The Wisconsin Card Sorting Test [WCST, @heatonWisconsinCardSorting1993] is a
widely used test of set-shifting ability which was developed to measure
flexibility of human thought and the ability to shift cognitive strategy in
response to changing contingencies. The WCST is designed to measure executive
functioning including attentional set shifting, task/rule switching or reversal,
and working memory abilities. The assessment requires the participants to sort
64 cards according to color (red, blue, yellow or green), shape (cross, circle,
triangle or star) or number of figures (one, two, three or four). Over the
course of the task, the sorting rule discreetly changes from color to shape or
number of figures, without the participants being informed. Participants must
modify their predictions and choices accordingly and sort the cards according to
the new sorting rule: they receive feedback for their response (correct or
incorrect), which should enable them to improve with implicit rule extraction
[@nelsonModifiedCardSorting1976]. The final score used was the percentage of
correct sorts after 64 trials. The WCST, scored according to the percentage (or
number) of correct sorts, exhibits satisfying split-half reliability
[Spearman-Brown *r* = .90, @koppReliabilityWisconsinCard2021].

The reading comprehension task assesses both explicit literal comprehension and
inferential comprehension skills and was designed for assessment in adult
readers [@brethesTextReadingFluency2022]. Reading comprehension is a complex
cognitive activity that involves a number of skills including word recognition
skills, grammar, semantic and general knowledge, working memory and reasoning
skills as well as inference-making abilities. This reading comprehension task
was composed of three texts drawn from the French daily newspaper *Le Monde*,
all dealing with the destruction of the Great Barrier Reef and its various
causes. The participant had to read each text without time constraints, then
answer eight questions: four questions about explicit literal comprehension and
four inferential questions about the comprehension of the implicit information
in the texts, among which two examined text-connecting inferences and two
examined knowledge-based inferences. While text-connecting inference skills
required participants to integrate text information in order to establish local
cohesiveness, knowledge-based inference skills required participants to
establish links between the text content and their own personal knowledge. The
questions were also divided between two question formats: open questions and
multiple-choice questions. Participants were not allowed to refer to the text
when answering the questions. In all, the test contained 20 questions whose
answers were scored by the experimenter, with 2 points for complete answers, 1
point for incomplete answers and 0 for incorrect answers. The maximum score was
therefore 40 points. The scoring was carried out manually, using double scoring
by the first two authors of the present article, blind to the groups and
participants. Cronbach’s $\alpha$ of the task is 0.78.

## Experimental design and procedure

The experiment was administered online via a JATOS server
[@langeJustAnotherTool2015]. It was programmed using SurveyJS and jsPsych
[@leeuwJsPsychEnablingOpenSource2023], open-source JavaScript libraries
dedicated to the creation of questionnaires and experiments respectively, as
well as OpenSesame [@mathotOpenSesameOpensourceGraphical2012], a graphical
interface for the construction of behavioural experiments. The link to the
experiment was emailed individually to each volunteer participant and could only
be used once per participant.

All participants were subjected to the same study design and task sequence.
Before the first questionnaires, participants gave their consent, then
demographic data were collected (first language, age, gender, occupation,
education and field of study, vision). Due to the length of the protocol (Median
time spent = 84.58 min, Median Absolute Deviation = 27.51 min), the experiment
was structured with instructions accompanied by pages of explanations to
reinforce engagement and focus (e.g., inviting people to "dive into their minds"
or "test their abilities"). No text mentioned the word aphantasia, to avoid the
stigma, bias or preconceived ideas specifically associated with this term [see
@cabbaiInvestigatingRelationshipsTrait2023;
@monzelAphantasiaFrameworkNeurodivergence2023]. The experiment started with the
VVIQ, followed by the Raven matrices, the WCST, the OSIVQ, the SRI, the reverse
Corsi blocks, the Similarities test, the Reading comprehension task, the reverse
digit span, and ended with the Psi-Q. None of the tasks had a time limit, but
participants were instructed to respond as soon as they had the answer while
maintaining accuracy, with the aim of reducing the total duration of the
experiment for them. However, given that the experiment was long, that the
participants were not monitored due to the online format, and that the
instructions did not place particular emphasis on speed of response as a key
aspect, response times were not analysed.

## Analyses

All analyses were conducted using the R statistical language [@R-base] on
RStudio [@Rstudio]. Data curation was handled in R with packages from the
*tidyverse* collection [@tidyverse2019]. All visualisations were produced with
the packages *ggplot2* [@ggplot22016], *factoextra* [@R-factoextra], *see*
[@see2021], *superb* [@superb2021] and *patchwork* [@R-patchwork].

Bayesian modelling used throughout the analyses was conducted using the R
packages *rstanarm* [@R-rstanarm], *BayesFactor* [@R-BayesFactor] and
*bayestestR* [@bayestestR2019], and unless otherwise stated default parameter
values were used. The R package *emmeans* [@R-emmeans] was used for marginal
estimates and contrast analyses. For all tests, the statistic reported,
$log(BF_{10})$, quantifies the relative "weight of evidence" in favour of the
hypothesis $H_{1}$ (e.g., the effect of a factor), against the null hypothesis
$H_{0}$ [@goodWeightEvidenceBrief1985]. According to Jeffrey's scale thresholds
[see @kassBayesFactors1995], $log(BF_{10}) \in [0;0.5[$ = "*Barely worth
mentioning*"; $\in [0.5;1[$ = "*Substantial evidence*"; $\in [1;2[$ = "*Strong
evidence*"; $\in [2;+\infty[$ = "*Decisive evidence*". The same negative
thresholds apply for the weight of evidence in favour of $H_0$.

# Results

```{r}
#| label: compute-what-we-need-for-the-manuscript
#| include: false

library(aphantasiaCognitiveClustering)

df     <- study_data
models <- models_list
cors   <- df |> correlate_vars(partial = TRUE)

df2 <- merge_clusters(
  df_raw     = study_data,
  df_red     = scale_reduce_vars(study_data),
  clustering = cluster_selected_vars(study_data)
)

lives <- list(
  group      = get_association_models(df2, group),
  cluster    = get_association_models(df2, cluster),
  subcluster = get_association_models(df2, subcluster)
)

# All numerical values used in the manuscript ----------------------------------

# Shorthand helper function to round numbers
r <- function(x, d = 2) round(x, d)

# Age
mean_age <- r(mean(df$age),1)
sd_age   <- r(sd(df$age),1)
min_age  <- min(df$age)
max_age  <- max(df$age)
m_age_aph  <- r(mean(df[df$group=="Aphantasic",]$age), 1)
sd_age_aph <- r(sd(df[df$group=="Aphantasic",]$age), 1)
m_age_con  <- r(mean(df[df$group=="Control",]$age), 1)
sd_age_con <- r(sd(df[df$group=="Control",]$age), 1)

# N
n_fem   <- nrow(df[df$sex == "f",])
n_men   <- nrow(df[df$sex == "m",])
n_oth   <- nrow(df[df$sex == "other",])
n_aph   <- nrow(df[df$group == "Aphantasic",])
n_con   <- nrow(df[df$group == "Control",])
n_aph_m <- nrow(df[df$group == "Aphantasic" & df$sex == "m",])
n_aph_f <- nrow(df[df$group == "Aphantasic" & df$sex == "f",])
n_con_m <- nrow(df[df$group == "Control" & df$sex == "m",])
n_con_f <- nrow(df[df$group == "Control" & df$sex == "f",])
n_con_o <- nrow(df[df$group == "Control" & df$sex == "other",])
n_clu_a <- nrow(df2[df2$cluster == "A (Aphant.)",])
n_clu_b <- nrow(df2[df2$cluster == "B (Mixed)",])
n_clu_c <- nrow(df2[df2$cluster == "C (Control)",])
n_clu_b_aph <- nrow(df2[df2$cluster == "B (Mixed)" & df$group == "Aphantasic",])
n_clu_b_con <- nrow(df2[df2$cluster == "B (Mixed)" & df$group == "Control",])

# VVIQ
m_vviq_aph  <- r(mean(df[df$group=="Aphantasic",]$vviq), 1)
sd_vviq_aph <- r(sd(df[df$group=="Aphantasic",]$vviq), 1)
m_vviq_con  <- r(mean(df[df$group=="Control",]$vviq), 1)
sd_vviq_con <- r(sd(df[df$group=="Control",]$vviq), 1)

# Education, fields, occupation
group_edu_bf <- lives$group$log_bf10[1]
group_fld_bf <- lives$group$log_bf10[2]
group_occ_bf <- lives$group$log_bf10[3]
clust_edu_bf <- lives$cluster$log_bf10[1]
clust_fld_bf <- lives$cluster$log_bf10[2]
clust_occ_bf <- lives$cluster$log_bf10[3]

# VVIQ groups
group_log_bfs    <- models$group_models$`$log(BF_{10})$`
group_vviq_bf    <- group_log_bfs[1] |> r(2)
group_osv_o_bf   <- group_log_bfs[2] |> r(2)
group_osv_s_bf   <- group_log_bfs[3] |> r(2)
group_osv_v_bf   <- group_log_bfs[4] |> r(2)
group_psi_vis_bf <- group_log_bfs[5] |> r(2)
group_psi_aud_bf <- group_log_bfs[6] |> r(2)
group_psi_sme_bf <- group_log_bfs[7] |> r(2)
group_psi_tas_bf <- group_log_bfs[8] |> r(2)
group_psi_tou_bf <- group_log_bfs[9] |> r(2)
group_psi_sen_bf <- group_log_bfs[10] |> r(2)
group_psi_fee_bf <- group_log_bfs[11] |> r(2)
group_rav_bf     <- group_log_bfs[12] |> r(2)
group_sri_bf     <- group_log_bfs[13] |> r(2)
group_dig_bf     <- group_log_bfs[14] |> r(2)
group_spa_bf     <- group_log_bfs[15] |> r(2)
group_sim_bf     <- group_log_bfs[16] |> r(2)
group_wcs_bf     <- group_log_bfs[25] |> r(2)
group_rea_bf     <- group_log_bfs[26] |> r(2)

# Additional detail for the OSIVQ-V
group_osv_v_delta <- models$group_models$`Difference ($\\Delta$)`[4] |> r(2)
group_osv_v_95cri <- models$group_models$`95% CrI`[4]

# Relevant partial correlations
# Visual
cor_vviq_psi_v <- cors[
  cors$Parameter1 == "VVIQ" & cors$Parameter2 == "Psi-Q\nVisual",
]$r |> r(2)
cor_osv_o_psi_v <- cors[
  cors$Parameter1 == "OSIVQ\nObject" & cors$Parameter2 == "Psi-Q\nVisual",
]$r |> r(2)
# Spatial
cor_osv_s_sri <- cors[
  cors$Parameter1 == "OSIVQ\nSpatial" & cors$Parameter2 == "SRI",
]$r |> r(2)
# Sensory
cor_sme_tas <- cors[
  cors$Parameter1 == "Psi-Q\nSmell" & cors$Parameter2 == "Psi-Q\nTaste",
]$r |> r(2)
cor_tas_tou <- cors[
  cors$Parameter1 == "Psi-Q\nTaste" & cors$Parameter2 == "Psi-Q\nTouch",
]$r |> r(2)
cor_tou_sen <- cors[
  cors$Parameter1 == "Psi-Q\nTouch" & cors$Parameter2 == "Psi-Q\nSensations",
]$r |> r(2)
cor_sen_fee <- cors[
  cors$Parameter1 == "Psi-Q\nSensations" & cors$Parameter2 == "Psi-Q\nFeelings",
]$r |> r(2)
# Raven + Digit
cor_rav_dig <- cors[
  cors$Parameter1 == "Raven\nMatrices" & cors$Parameter2 == "Digit\nspan",
]$r |> r(2)

# Clusters
clust_log_bfs <- models$cluster_models$`$log(BF_{10})$`
# Visual & sensory
clust_vis_sens_min_bf  <- clust_log_bfs[c(49:51, 55:57)] |> min()
# Spatial
clust_spa_b_ac_min_bf  <- clust_log_bfs[c(58, 60)] |> min()
clust_spa_ac_bf        <- clust_log_bfs[59]
# Verbal
clust_ver_a_bc_min_bf  <- clust_log_bfs[c(61, 62)] |> min()
clust_ver_bc_bf        <- clust_log_bfs[63]
# Reasoning and spatial span
clust_res_bc_min_bf    <- clust_log_bfs[c(69, 72)] |> min()
clust_res_a_bc_max_bf  <- clust_log_bfs[c(67, 68, 70, 71)] |> max()
# Raven + Digit
clust_rav_dig_bf       <- clust_log_bfs[c(64:66)] |> max()
# Auditory
clust_aud_ab_bf        <- clust_log_bfs[52]
clust_aud_bc_bf        <- clust_log_bfs[54]
# WCST
clust_wcs_a <- models$cluster_models$`A (Aphant.)`[73]
clust_wcs_b <- models$cluster_models$`B (Mixed)`[73]
clust_wcs_c <- models$cluster_models$`C (Control)`[73]
# Reading
clust_rea_a <- models$cluster_models$`A (Aphant.)`[76]
clust_rea_b <- models$cluster_models$`B (Mixed)`[76]
clust_rea_c <- models$cluster_models$`C (Control)`[76]
# Highest BF for WCST/Reading
clust_wcs_rea_bf <- clust_log_bfs[c(73:75, 76:78)] |> max()

# Sub-clusters
subcl_log_bfs <- models$subcluster_models$`$log(BF_{10})$`
# Between controls
subcl_vis_bc_c_bf <- subcl_log_bfs[102]
subcl_aud_bc_c_bf <- subcl_log_bfs[108]
subcl_sen_bc_c_bf <- subcl_log_bfs[114]
subcl_spa_bc_c_bf <- subcl_log_bfs[120]
subcl_ver_bc_c_bf <- subcl_log_bfs[126]
subcl_rav_bc_c_bf <- subcl_log_bfs[132]
subcl_rea_bc_c_bf <- subcl_log_bfs[138]
subcl_spn_bc_c_bf <- subcl_log_bfs[144]
# Between aphants
subcl_vis_ba_a_bf <- subcl_log_bfs[97]
subcl_aud_ba_a_bf <- subcl_log_bfs[103]
subcl_sen_ba_a_bf <- subcl_log_bfs[109]
subcl_spa_ba_a_bf <- subcl_log_bfs[115]
subcl_ver_ba_a_bf <- subcl_log_bfs[121]
subcl_rav_ba_a_bf <- subcl_log_bfs[127]
subcl_rea_ba_a_bf <- subcl_log_bfs[133]
subcl_spn_ba_a_bf <- subcl_log_bfs[139]
```

The final sample comprised 96 participants ($M_{age}$ = `r mean_age`, $SD_{age}$
= `r sd_age`, range$_{age}$ = \[`r min_age`, `r max_age`\], `r n_fem` females,
`r n_men` males, `r n_oth` another gender). The most widely used criterion in
studies on aphantasia to identify the condition is a score inferior to 32 on the
Vividness of Visual Imagery Questionnaire [@marksVisualImageryDifferences1973].
After applying this criterion, we identified `r n_aph` individuals with
aphantasia in the sample ($M_{VVIQ}$ = `r m_vviq_aph`, $SD_{VVIQ}$ =
`r sd_vviq_aph`, $M_{age}$ = `r m_age_aph`, $SD_{age}$ = `r sd_age_aph`,
`r n_aph_f` females, `r n_aph_m` males) and 51 controls ($M_{VVIQ}$ =
`r m_vviq_con`, $SD_{VVIQ}$ = `r sd_vviq_con`, $M_{age}$ = `r m_age_con`,
$SD_{age}$ = `r sd_age_con`, `r n_con_f` females, `r n_con_m` males, `r n_con_o`
another gender).

## VVIQ groups analysis

### Education and occupation

Participants' level of education, field of study, and occupation were analysed
to detect any association with the grouping factor. Levels of education were
coded using the equivalent of the French levels in the International Standard
Classification of Education (ISCED), i.e., *Upper secondary*, *Post-secondary*,
*Bachelor*, *Master*, and *Doctorate*. Fields of study have been coded according
to the 10 broad categories defined by the ISCED-F 2013 (ISCED: Fields of
Education and Training). Occupations have been coded according to the sub-major
groups of the International Standard Classification of Occupations (ISCO-08) for
an appropriate level of precision given our sample size. Nine occupational
groups were identified in the sample. Bayes factors for independence were
calculated to evaluate the association between groups and each demographic
variable [see @gunelBayesFactorsIndependence1974]. The tests found evidence
against a relationship between groups and levels of education ($log(BF_{10})$ =
`r group_edu_bf`), groups and fields of study ($log(BF_{10})$ =
`r group_fld_bf`), or groups and occupation ($log(BF_{10})$ = `r group_occ_bf`).

### Questionnaire and task results

The measured variables were the scores on the VVIQ, the three OSIVQ scales, the
seven Psi-Q scales, the Raven matrices, the SRI, the Similarities Test, the
reverse spatial and verbal spans, the WCST and the Reading comprehension task.
Linear models have been fitted to the various variables to model them with
participant groups as categorical predictors and age as a continuous covariate
so as to control for the potential influence of the latter. Contrast analyses
were thereafter conducted to assess the differences between the groups. All
score differences (hereinafter referred to as $\Delta$) and their 95% Credible
Intervals are reported in @tbl-g-results along with the $log(BF_{10})$
quantifying the weight of evidence in favour of a non-null difference between
the groups.

Individuals with aphantasia had lower scores than controls in all visual imagery
scales (VVIQ: $log(BF_{10})$ = `r group_vviq_bf`; OSIVQ-Object: $log(BF_{10})$ =
`r group_osv_o_bf`; Psi-Q Visual: $log(BF_{10})$ = `r group_psi_vis_bf`), but
also on all other sensory imaging modalities evaluated by the Psi-Q
($log(BF_{10}) \in [14; 31]$ for all modalities). All means and contrasts
between the groups are represented with their distributions in @fig-g-violins.
Apart from sensory imagery, evidence was found in favour of a difference between
the groups on the verbal scale of the OSIVQ, with individuals with aphantasia
scoring higher than controls ($\Delta$ = `r group_osv_v_delta`, 95% CrI =
`r group_osv_v_95cri`, $log(BF_{10})$ = `r group_osv_v_bf`). No differences
between the groups were found on all other variables: the statistical analyses
showed evidence against a difference on the spatial scale of the OSIVQ
($log(BF_{10})$ = `r group_osv_s_bf`), against differences in Raven matrices
scores ($log(BF_{10})$ = `r group_rav_bf`), SRI scores ($log(BF_{10})$ =
`r group_sri_bf`), spatial span ($log(BF_{10})$ = `r group_spa_bf`), digit span
($log(BF_{10})$ = `r group_dig_bf`), Similarities test scores ($log(BF_{10})$ =
`r group_sim_bf`), Reading comprehension scores ($log(BF_{10})$ =
`r group_rea_bf`) and WCST scores ($log(BF_{10})$ = `r group_wcs_bf`).

```{r}
#| label: fig-g-violins
#| fig-cap: "Standardised scores of the two VVIQ groups on all the 
#| questionnaires and tasks. The scores have been rescaled between 0 and 1 to be 
#| represented on the same scale. The coloured shapes represent the distribution 
#| of the scores in each group. The coloured dots represent the mean of each 
#| group, while the bars represent the standard deviations. The stars represent 
#| weight of evidence thresholds in favour of an effect of the Group: \\* = 
#| '*Substantial evidence*', \\** = '*Strong evidence*', \\*** = '*Decisive 
#| evidence*'."
#| fig-width: 11
#| fig-height: 5

df |> 
  scale_vars() |> 
  get_longer() |>
  filter_study_variables("original") |> 
  plot_score_violins(
    txt_big  = 12,
    txt_mid  = 10,
    txt_smol = 8,
    dot_big  = 0.5,
    lw_big   = 0.5,
    lw_smol  = 0.5
  )
```

The VVIQ model—i.e., the division of the sample into two VVIQ groups of
individuals with aphantasia and controls—therefore had very little explanatory
power on task performance. However, large inter-individual variances were
observed in various outcomes, as evidenced by the spread of the outcomes'
distributions and several bimodal distributions (e.g., distributions of the
OSIVQ-Verbal, SRI, or Reading comprehension scores, see @fig-g-violins). These
unexplained differences suggested the existence of an underlying structure in
our sample, thus requiring a better model with more relevant groups to account
for them in light of our data. We studied this hypothesis by searching for
sub-groups in the sample using data-driven unsupervised clustering.

## Cluster analysis

### Correlation structure and variable selection

The selection of relevant variables for clustering is essential for good model
fit and interpretation of the results [@fopVariableSelectionMethods2018;
@zakharovApplicationKmeansClustering2016]. Having an adequate number of
dimensions (variables) for a given sample size is also crucial to increase the
quality of the clustering [@psutkaSampleSizeMaximumlikelihood2019]. The
identification and reduction of *redundant variables* is particularly important,
so as not to distort the relative weight of each latent variable in the
clustering process. If two variables represent the same concept, that concept
would be represented twice in the data and hence get twice the weight as all the
other variables. The final solution could be skewed in the direction of that
concept, which would considerably compromise the relevance of the model for
understanding variable importance [@kyriazosDealingMulticollinearityFactor2023].
In the present analysis, this issue particularly affected sensory imagery, which
was represented by nine highly correlated variables (VVIQ, OSIVQ-Object, and the
seven Psi-Q modalities, Pearson's $r \in [0.65, 0.94]$ for every pairwise
correlation) that were likely to reflect very similar constructs, as opposed to
the remaining nine variables. Several methods exist to deal with such
multicollinearity problems. In our low-dimensional setting, we chose to merge
variables by averaging them to maintain interpretability while enhancing the
stability of the model [@kyriazosDealingMulticollinearityFactor2023]. To choose
which variables to merge, we analysed the relationships between the variables
using partial correlations. Partial correlations measure the degree of
association between two variables while controlling for the effect of other
potentially confounding covariates [@abdiPartSemiPartial2007]. This procedure
allows to identify the strongest unbiased links between variables and prevents
misinterpretation of spurious correlations.

We computed all the partial correlations between the 18 variables (see
@fig-corrs) and chose to merge the significantly correlated variables after a
Bonferroni correction (multiplying the *p*-values by the number of comparisons).
This resulted in the creation of four new reduced variables. First, the three
subscales related to visual imagery, i.e., the VVIQ, the OSIVQ-Visual and Psi-Q
Visual, were associated (VVIQ - Psi-Q Visual: *r* = `r cor_vviq_psi_v`, *p* $<$
0.001; OSIVQ-Object - Psi-Q Visual: *r* = `r cor_osv_o_psi_v`, *p* $<$ 0.05).
They have been standardised between 0 and 1, weighted by their number of items
(16, 15 and 3 respectively) and merged into a single "*Visual imagery*" variable
to obtain as balanced a continuous measure of imagery as possible. Second, the
OSIVQ-Spatial and the score of the SRI, i.e., subjective and objective spatial
imagery, were associated (*r* = `r cor_osv_s_sri`, *p* $<$ 0.05). They have been
standardised and merged in a single "*Spatial imagery*" variable. Third, five
Psi-Q sensory imagery subscales (Smell, Taste, Touch, Sensations and Feelings)
were associated (Psi-Q Smell - Taste: *r* = `r cor_sme_tas`, *p* $<$ 0.001;
Taste - Touch: *r* = `r cor_tas_tou`, *p* $<$ 0.05; Touch - Sensations: *r* =
`r cor_tou_sen`, *p* $<$ 0.01; Sensations - Feelings: *r* = `r cor_sen_fee`, *p*
$<$ 0.05). They have been standardised and merged into a single "*Sensory
imagery*" variable. Fourth, the score for the Raven matrices and the Digit span
were associated (*r* = `r cor_rav_dig`, *p* $<$ 0.05). They have been
standardised and merged in a common "*Raven + Digit*" variable. Based on the
link established between the reverse digit span and measures of intellectual or
executive functions [@groegerMeasuringMemorySpan1999], we interpreted this
variable theoretically as a proxy for general cognitive performance.

```{r}
#| label: fig-corrs
#| fig-cap: "Correlation matrix and undirected graph representing the partial 
#| correlations between the 18 variables. The stars in the matrix represent 
#| p-value thresholds: * : p $<$ 0.05, ** : p $<$ 0.01, *** : p $<$ 0.001, after 
#| a Bonferroni correction. The links in the graph represent significant partial 
#| correlations. The coloured (non-black) nodes highlight the variables used for 
#| clustering, some of which are the result of merging of several correlated 
#| variables. Light blue: Visual imagery; dark blue: Sensory imagery; light 
#| orange: Spatial imagery; green: Verbal strategies; pink: Raven + Digit; dark 
#| orange: Verbal reasoning."
#| fig-width: 14
#| fig-height: 9

plot_score_cor_joint(
  cors,
  size_axis   = 10,
  matrix_text = 8,
  node_size   = 24,
  node_text_size  = 8,
  label_text_size = 4
  )
```

Finally, three variables were not included in the clustering. The Psi-Q Auditory
was the only Psi-Q subscale that was not associated with other variables. As it
comprises only three items, this variable was not included in the clustering to
avoid giving it undue importance. The WCST and Reading comprehension scores were
not used either, as these tasks are designed to evaluate higher-level abilities
that operate at more integrated levels of cognition. Executive functioning and
reading comprehension inextricably involve a mix of working memory, reasoning
and attention [@heatonWisconsinCardSorting1993; @kongsWCST64WisconsinCard2000;
@suggateMentalImagerySkill2022], and are likely to integrate many redundant
processes with the other assessments. Instead, these variables were used *a
posteriori* as testing variables to assess the generalisability of the cluster
model to external variables, i.e., to a related sensory imagery subscale for the
Psi-Q Auditory, and to complex cognitive tasks for the other two. This decision
was not planned before the study, but it was decided before conducting the
cluster analysis based on variable reduction and theoretical considerations.

This entire selection procedure allowed to reduce the variable space to seven
dimensions, estimated by @psutkaSampleSizeMaximumlikelihood2019 to yield a good
accuracy of parameter recovery for model-based clustering (see next section) on
a sample *N* = 96. As a result, other variables were not modified to keep as
much information as possible. For the sake of clarity, several scores have been
renamed to reflect what they assess. The OSIVQ-Verbal score was identified as
the propensity to use *Verbal strategies* for information processing, in line
with the definition of this sub-scale [see
@blazhenkovaNewObjectspatialverbalCognitive2009]. The Similarities test score
was identified as a *Verbal Reasoning* variable. The clustering process was
therefore conducted on the seven following variables: *Visual imagery*, *Sensory
imagery*, *Spatial imagery*, *Verbal strategies*, *Raven + Digit span*, *Verbal
reasoning* and *Spatial span*. To model variables using the same scale, data
were normalized between 0 and 1 from their respective scales, as recommended by
@zakharovApplicationKmeansClustering2016.

### Model-based clustering and number of clusters

A model-based method was chosen for clustering. In this approach, clustering
aims at modelling distributions with mixtures of multivariate Gaussian
distributions [@steinleyEvaluatingMixtureModeling2011]. Finite Gaussian mixture
models (GMM) attempt to determine the underlying population groups that produced
the observed data, each cluster being a distribution with its own centre and
spread. The resulting model is then used to compute the probability of each
observation belonging to a cluster. Although discrete (k-means) or hierarchical
clustering methods are frequently used in psychology
[@zakharovApplicationKmeansClustering2016], probabilistic mixture modelling
approaches have proven to be more powerful and parsimonious with partially
overlapping, non-spherical, multivariate normal distributions, and small sample
sizes, all of which are common in psychology experiments
[@dalmaijerStatisticalPowerCluster2022].

Given that very little information is available on the clusters, the estimation
of the GMM proceeds in steps, alternating between (1) estimating the posterior
probability of each observation belonging to each cluster with a fixed set of
parameters and (2) updating the estimates of the parameters by fixing the
probability of cluster membership for each observation
[@steinleyEvaluatingMixtureModeling2011]. This iterative procedure continues
until the model converges on stable clusters. The standard method for this
estimation is the expectation-maximisation algorithm [EM,
@dempsterMaximumLikelihoodIncomplete1977][^1]. In the present study, the
*mclust* R package [@mclust2023] was used to conduct GMM clustering. The
estimation procedure for the mixture of clusters in the GMM requires knowledge
of the number of clusters and their distributional form. The determination of
these parameters was done using the Bayesian Information Criterion (BIC)
implemented in the *mclust* package.

[^1]: Mathematical and technical details can be found in the documentation for
    the *mclust* R package [@mclust2023], among others.

```{r}
#| label: fig-bic
#| fig-cap: "Comparison of the goodness of fit of different mixture models used 
#| for clustering as a function of model type and number of components. A high 
#| BIC indicates a good model fit. The three-letter acronyms describe the 
#| components of the mixture models. The first letter describes the volume of 
#| the components, the second their shape and the last their orientation. 
#| E = equal, V = variable. Acronyms ending with ‘II’ indicate mixtures of 
#| spherical components, those ending with ‘I’ indicate mixtures of diagonal 
#| components and those without 'I' indicate mixtures of ellipsoidal 
#| components."
#| fig-width: 8
#| fig-height: 6

df |> 
  cluster_selected_vars() |> 
  plot_clusters_bic(
    txt_big  = 10,
    txt_mid  = 9,
    txt_smol = 8,
    size = 0.5
  )
```

There are no generally accepted rules regarding minimum sample sizes in
clustering procedures. For model-based clustering procedures
@dalmaijerStatisticalPowerCluster2022 recommended *N* = 20 to *N* = 30 per
expected cluster with medium to large effect sizes (i.e., cluster separation),
which would translate into two to four clusters in our sample. Given the present
dataset, the comparison of various GMM types with different number of clusters
using the BIC showed that the best solution was a model with three ellipsoidal
clusters of varying shapes, equal volume and orientations (EVE model, see
@fig-bic).

### Clustering results

The profiles of the three clusters obtained are presented in
@fig-cluster-radar-plots (left panel). **Cluster A** was made up exclusively of
individuals with aphantasia ($N_A$ = `r n_clu_a`), **Cluster B** was mixed
($N_B$ = `r n_clu_b`), comprising `r n_clu_b_con` controls and `r n_clu_b_aph`
individuals with aphantasia, while **Cluster C** was composed solely of control
participants ($N_C$ = `r n_clu_c`). Bayes factors for independence found no
association between clusters and levels of education ($log(BF_{10})$ =
`r clust_edu_bf`), clusters and fields of study ($log(BF_{10})$ =
`r clust_fld_bf`), or clusters and occupation ($log(BF_{10})$ =
`r clust_occ_bf`).

```{r}
#| label: fig-cluster-radar-plots
#| fig-cap: "Profiles of the clusters obtained through model-based clustering. 
#| **Left:** Polar plot representing the standardised means (points) and 
#| standard errors (error bars) of the three clusters on the seven clustering 
#| variables and the three external variables (Auditory imagery, WCST and 
#| Reading comprehension). Cluster A is exclusively composed of individuals 
#| with aphantasia, Cluster C is exclusively composed of control participants, 
#| and Cluster B is mixed. **Right:** Same polar plot with Cluster B divided 
#| into two sub-clusters, 'B-Aphant.' with the 13 individuals with aphantasia 
#| in B, and 'B-Control' with the 17 control participants in B."
#| fig-width: 10
#| fig-height: 5

# Attaching superb is necessary to avoid bugs with the 1.0
library(superb)

df2_long <- 
  df2 |> 
  scale_vars() |> 
  get_longer() |> 
  filter_study_variables("reduced")

plot_score_radars(
  df2_long, Cluster,  
  txt_big  = 9,
  txt_smol = 7,
  dot_size = 1.5,
  lw_line  = 0.5,
  lw_error = 0.5,
  y_off    = 57, # to center the y axis text
  r_off = 6) +
  plot_score_radars(
    df2_long, Subcluster, 
    txt_big  = 9,
    txt_smol = 7,
    dot_size = 1.5,
    lw_line  = 0.5,
    lw_error = 0.5,
    y_off    = 57, # to center the y axis text
    l_off = 6)
```

Cluster scores were modelled with the clusters as predictors using the same
linear Bayesian modelling as described above for modelling variables with the
groups. All detailed pairwise differences between the clusters and their 95%
Credible Intervals are reported in @tbl-c-contrasts along with weights of
evidence in favour of differences between clusters. The three clusters had very
distinctive features on the visual, sensory, spatial and verbal dimensions of
their cognitive profiles. As seen, for Visual and Sensory imagery, Controls
(cluster C) had much higher scores than the mixed cluster (cluster B) which
itself had higher scores than the Aphantasic cluster (cluster A; all
$log(BF_{10}) \geq$ `r clust_vis_sens_min_bf`). Cluster B had higher scores than
the other two clusters in Spatial imagery (both $log(BF_{10}) \geq$
`r clust_spa_b_ac_min_bf`), which did not differ from each other ($log(BF_{10})$
= `r clust_spa_ac_bf`). In turn, cluster A had higher scores than the other two
in Verbal strategies (both $log(BF_{10}) \geq$ `r clust_ver_a_bc_min_bf`), for
which B and C did not differ ($log(BF_{10})$ = `r clust_ver_bc_bf`). Else,
cluster B outperformed cluster C in verbal reasoning and spatial span (both
$log(BF_{10}) \geq$ `r clust_res_bc_min_bf`), but no differences were found
between cluster A and the other two on these scores (all $log(BF_{10}) \leq$
`r clust_res_a_bc_max_bf`). Finally, for the Raven + Digit span variable, the
differences between the clusters were negligible (all $log(BF_{10}) \leq$
`r clust_rav_dig_bf`).

We also modelled the variables excluded from the clustering procedure, i.e.,
Auditory imagery, WCST and reading comprehension scores, to assess how the
cluster patterns transferred to external variables. They are seen at the bottom
of @tbl-c-contrasts. The imagery patterns were maintained for Auditory imagery,
where cluster C scored higher than B ($log(BF_{10})$ = `r clust_aud_bc_bf`),
which in turn scored higher than A ($log(BF_{10})$ = `r clust_aud_ab_bf`).
However, although the mean cluster scores on the WCST and reading comprehension
followed the same trend as the other cognitive tasks (i.e. B $>$ A $>$ C — WCST:
$M_{B}$ ($SD_{B}$) = `r clust_wcs_b`; $M_{A}$ ($SD_{A}$) = `r clust_wcs_a`;
$M_{C}$ ($SD_{C}$) = `r clust_wcs_c` — Reading comprehension: $M_{B}$ ($SD_{B}$)
= `r clust_rea_b`; $M_{A}$ ($SD_{A}$) = `r clust_rea_a`; $M_{C}$ ($SD_{C}$) =
`r clust_rea_c`), there was evidence for an absence of differences between
clusters (highest $log(BF_{10})$ = `r clust_wcs_rea_bf`).

Dividing the controls and individuals with aphantasia within cluster B into two
"sub-clusters" provided further insights on the profiles in the sample (see
@fig-cluster-radar-plots, right panel; details of all 60 pairwise comparisons
can be found in the extended analysis report on the OSF at
<https://osf.io/7vsx6/>). This allowed to assess imagery differences that might
have been biased by averaging imagery scores in this mixed cluster. First,
comparisons between controls from B (noted $B_{C}$) and from C showed that C had
higher visual imagery than $B_{C}$ ($log(BF_{10})$ = `r subcl_vis_bc_c_bf`), but
comparable sensory imagery ($log(BF_{10})$ = `r subcl_sen_bc_c_bf`) and auditory
imagery scores ($log(BF_{10})$ = `r subcl_aud_bc_c_bf`). As observed above,
B-Controls had higher spatial imagery ($log(BF_{10})$ = `r subcl_spa_bc_c_bf`),
spatial span ($log(BF_{10})$ = `r subcl_spn_bc_c_bf`) and verbal reasoning
scores ($log(BF_{10})$ = `r subcl_rea_bc_c_bf`), but no differences in verbal
strategies ($log(BF_{10})$ = `r subcl_ver_bc_c_bf`) or Raven + Digit span
($log(BF_{10})$ = `r subcl_rav_bc_c_bf`). These comparisons show that the main
imagery differences between the B and C controls lie in visual and spatial
imagery, indicating that the differences in performance observed in the tasks
could be related to the balance between the two, a high visual imagery being
associated with lower performance.

Last, comparisons between individuals with aphantasia from A and from B (noted
$B_{A}$) confirmed their differences in verbal strategies in favour of A
($log(BF_{10})$ = `r subcl_ver_ba_a_bf`), and in spatial imagery in favour of
$B_{A}$ ($log(BF_{10})$ = `r subcl_spa_ba_a_bf`), along with an absence of
differences in Raven + Digit span ($log(BF_{10})$ = `r subcl_rav_ba_a_bf`),
spatial span ($log(BF_{10})$ = `r subcl_spn_ba_a_bf`), and verbal reasoning
($log(BF_{10})$ = `r subcl_rea_ba_a_bf`). However, the contrasts also revealed
that, although the two had comparable visual imagery ($log(BF_{10})$ =
`r subcl_vis_ba_a_bf`), $B_{A}$ had higher sensory imagery than A
($log(BF_{10})$ = `r subcl_sen_ba_a_bf`). This difference also existed in
auditory imagery ($log(BF_{10})$ = `r subcl_aud_ba_a_bf`), which was not used as
a variable for clustering. The differences in verbal strategies, spatial
imagery, sensory and auditory imagery between A and $B_{A}$ aphants point to the
existence of aphantasia subgroups with major mental imagery and cognitive style
differences extending beyond visual imagery.

# Discussion

The present study investigated the cognitive profiles of individuals with and
without aphantasia and how these profiles influence their performance on various
cognitive tasks. To this end, we examined key aspects of subjective experiences,
including visual and sensory imagery, spatial imagery, and verbal strategies,
using questionnaires. Additionally, reasoning, working memory, executive
functioning, and reading comprehension were assessed through a battery of
behavioural tasks.

The initial analyses, comparing individuals with aphantasia to controls based on
conventional measures, revealed few significant differences in performance.
While individuals with aphantasia demonstrated reduced sensory imagery and a
stronger reliance on verbal strategies, differences in spatial imagery and other
cognitive tasks, such as reasoning or memory, were minimal. These findings
aligned with previous research emphasizing the limited impact of aphantasia in
many behavioural tasks [e.g., @bainbridgeQuantifyingAphantasiaDrawing2021;
@keoghVisualWorkingMemory2021; @knightMemoryImageryNo2022;
@pounderOnlyMinimalDifferences2022]. However, a clustering analysis using a
Gaussian Mixture Model (GMM) identified a cluster structure within the dataset.
By leveraging variables related to visual, sensory, and spatial imagery, verbal
strategies, and cognitive abilities (i.e., Raven matrices, digit span, spatial
span, and verbal reasoning), the algorithm suggested the presence of three
distinct cognitive profiles, each with unique characteristics in visual,
sensory, and spatial imagery, as well as verbal strategies. One cluster,
exclusively composed of controls, demonstrated high visual and sensory imagery,
strongly relying on visual-object representations. Another mixed cluster,
including both individuals with aphantasia and controls, exhibited strong
spatial imagery and performed best on spatial span and verbal reasoning tasks.
The final cluster consisted entirely of individuals with aphantasia, who
displayed multisensory aphantasia (reduced sensory, auditory, and visual
imagery) and a reliance on verbal strategies. This clustering revealed complex,
multidimensional patterns of cognitive abilities and imagery preferences that
extended beyond the binary categorization of aphantasia versus control, offering
a rich and nuanced picture of the relationships between mental imagery and
cognition.

If we focus only on the Control participants, the cluster analyses revealed
important subgroup distinctions. Controls could be divided into two subgroups:
one with highly vivid visual imagery but reduced spatial imagery (cluster C) and
another with the opposite pattern, characterized by stronger spatial imagery
(cluster $B_{C}$). These distinctions held important implications for task
performance. Individuals with higher spatial imagery outperformed their
counterparts with stronger visual imagery in tasks requiring reasoning and
working memory, such as spatial span and verbal reasoning tasks. Interestingly,
both groups performed similarly on tasks involving non-verbal reasoning (Raven
matrices) and reverse digit span, suggesting that the observed differences were
not due to overall disparities in cognitive ability but were specific to
differences in imagery style. These results emphasize the importance of spatial
imagery in abstract reasoning and suggest that vivid visual imagery may
sometimes impede these processes. This is consistent with the “visual imagery
impedance hypothesis” [@knauffVisualImageryCan2002], which posits that overly
detailed visual representations can disrupt reasoning by introducing irrelevant
information. These findings not only reinforce the relevance of spatial imagery
in cognitive profiles but also raise questions about its relationship with
visual imagery, highlighting potential trade-offs between visual and spatial
imagery capacities, a dynamic previously suggested by
@kozhevnikovTradeoffObjectSpatial2010.

The clustering approach also revealed important heterogeneity within the group
of individuals with aphantasia. While a shared absence of visual imagery defined
all participants with aphantasia, two distinct subgroups emerged: one that
relied heavily on spatial strategies (“spatialisers”) and another that depended
predominantly on verbal processing (“verbalisers”). Spatialisers demonstrated
preserved sensory and auditory imagery and demonstrated strong performance in
tasks that emphasized spatial representation, such as mental rotation and
spatial manipulation tasks. Verbalisers, by contrast, exhibited what could be
described as “multisensory aphantasia” reporting not only the absence of visual
imagery but also reduced or absent sensory and auditory imagery. This group
relied heavily on verbal strategies, with a preference for linguistic forms of
information processing. The distinction between “multisensory aphantasia” and
“visual-only aphantasia” aligns with a prior clustering study conducted by
@dawesMultisensorySubtypesAphantasia2023, which highlighted the heterogeneity of
aphantasia across larger samples. However, our findings uncover another relevant
dimension by linking these imagery profiles to different cognitive styles:
verbal versus spatial. The strong performance of spatialisers on spatial tasks
supports prior work suggesting that many classic "mental imagery tasks", such as
the Mental Rotation Task [@shepardMentalRotationThreeDimensional1971], or the
Paper Folding Test [@ekstromKitFactorreferencedCognitive1976] rely more heavily
on spatial than visual-object imagery [e.g.,
@kozhevnikovSpatialObjectVisualizers2005;
@blazhenkovaNewObjectspatialverbalCognitive2009;
@borstIndividualDifferencesSpatial2010; @kozhevnikovTradeoffObjectSpatial2010;
@haciomerogluObjectSpatialVisualizationVerbal2016;
@bledValidationFrenchVersion2021]. Furthermore, these findings on cognitive
styles provide insights into why individuals with aphantasia perform well on
spatial reasoning tasks, echoing earlier work on the amodal nature of spatial
representations [@johnson-lairdMentalModelsHuman2010] and spatial imagery in
aphantasia [@palermoCongenitalLackExtraordinary2022]. Nuances in spatial imagery
and cognitive styles among individuals with aphantasia may also be related to
the presence or absence of unconscious visual representations, a phenomenon
currently debated in the literature [see
@krempelAphantasiaInvoluntaryImagery2024; @murakiInsightsEmbodiedCognition2023;
@purkartAreThereUnconscious2024]. In this respect, unsupervised clustering could
unveil previously unrecognised heterogeneities within visual imagery groups,
offering novel insights into the cognitive architectures underlying mental
representations.

The spatial and verbal profiles observed in aphantasia profiles could also have
broader implications beyond cognitive processes and task performance.
@zemanPhantasiaPsychologicalSignificance2020 have noted a disproportionate
representation of individuals with aphantasia in STEM fields, which may reflect
the influence of these spatial profiles among individuals with visual
aphantasia. Indeed, numerous studies on the associations between the OSIVQ and
categories of occupation, study and activity have found correlations between the
three visual-object, spatial and verbal dimensions and areas of specialisation
in visual arts, science and humanities, respectively [for an extended review,
see @blazhenkovaTwoEyesBlind2019]. These patterns suggest that cognitive
profiles encompassing spatial and verbal dimensions could be relevant to
understand the consequences of aphantasia in ecological contexts, as supported
by recent findings linking OSIVQ-derived profiles to real-world problem-solving
abilities [@hofflerMoreEvidenceThree2017;
@chkhaidzeIndividualDifferencesPreferred2023]. Aphantasia may represent one
extreme among many in a larger, multidimensional cognitive spectrum, where
individual differences in imagery and cognitive style influence both cognitive
abilities, subjective experience and daily life.

Since the first systematic investigation of aphantasia
[@zemanLivesImageryCongenital2015], individuals with this condition have
reported “compensatory strengths in verbal, mathematical and logical domains”,
but our understanding of these cognitive strengths and weaknesses associated
with aphantasia remains incomplete. While the profiles identified in this study
offer a potential framework to explore these compensatory mechanisms further,
several limitations must be acknowledged. The use of an unsupervised clustering
algorithm allowed for the discovery of hidden structures in the data, but the
findings remain exploratory and require replication in larger, more diverse
samples. Moreover, the tasks used to assess mental imagery and reasoning were
selected to provide broad coverage of cognitive abilities but may not have
captured the full complexity of these constructs. Future research should aim to
refine the measurement of visual, spatial, and verbal imagery through the use of
tasks specifically designed to disentangle these dimensions. Furthermore, it
remains to be seen whether the profiles identified here can be validated with
other behavioural and neural phenomena, such as differences in brain activity
during visual or spatial imagery tasks.

Overall, the findings of this study suggest that variations in visual, spatial,
and verbal cognitive styles offer a rich framework for understanding the
“phantasia continuum”. This framework not only highlights the heterogeneity
within the aphantasia population but also situates aphantasia within a broader
spectrum of cognitive abilities. Ultimately, by moving beyond the binary
classification of aphantasia and exploring the multidimensional nature of
cognitive profiles, this study aims to contribute to a more nuanced and cohesive
understanding of the diversity of mental imagery and its cognitive implications.
In turn, the spectacular variability in inner experiences represents an
invaluable source of information into mental representations, which could answer
long-standing questions about their nature, from their modal and amodal
properties to their interaction with fundamental processes such as reasoning,
problem-solving or working memory.

# Research transparency statement {.unnumbered}

All the following elements required to reproduce the study and analyses are
publicly available on the Open Science Framework (<https://osf.io/7vsx6/>): all
online study materials; all anonymised primary data and pre-processed data; all
analysis code and notebooks with extensive commentary and supplementary
information on the exploratory analysis process and results. No artificial
intelligence assisted technologies were used in this research or the creation of
this article.

# Author contributions {.unnumbered}

Conceptualisation: MD, ST, EC, GP. Data curation: MD. Formal analysis: MD.
Funding acquisition: GP. Investigation: MD, ST. Methodology: MD, ST, DC, EC, GP.
Project administration: GP, EC. Resources: MD, ST, EC, DC. Software: MD.
Supervision: GP, EC. Visualisation: MD. Writing - Original Draft Preparation:
MD. Writing - Review & Editing: GP, DC, EC.

# Declaration of interests {.unnumbered}

None.



# References {.unnumbered}

::: {#refs}
:::



# Tables {.unnumbered}

```{r}
#| label: tbl-g-results
#| tbl-cap: "Means and standard deviations (SD) of the scores of each VVIQ group 
#| for every variable. The score differences, their 95% Credible Intervals 
#| (CrI) and weights of evidence are reported for each variable."

models$group_models |>
  dplyr::select(1:3, 8:10) |> 
  dplyr::filter(!(Variable %in% c(
    "Visual imagery",
    "Auditory imagery",
    "Sensory imagery",
    "Spatial imagery",
    "Verbal strategies",
    "Raven +\nDigit Span",
    "Non-verbal\nreasoning",
    "Verbal reasoning",
    "Spatial span std."
  ))) |> 
  dplyr::rename(
    `Control (n = 51)` = Control,
    `Aphantasia (n = 45)` = Aphantasic,
  ) |> 
  knitr::kable()
```



```{r}
#| label: tbl-c-contrasts
#| tbl-cap: "Pairwise comparisons between the three clusters for the seven 
#| clustering variables and the three external variables (Auditory imagery, 
#| WCST and Reading comprehension). The $log(BF_{10})$ quantifies the weight of 
#| evidence in favour of a difference between clusters."

models$cluster_models |> 
  dplyr::select(1, 8:11) |> 
  dplyr::filter(Variable %in% c(
    "Visual imagery",
    "Auditory imagery",
    "Sensory imagery",
    "Spatial imagery",
    "Verbal strategies",
    "Raven +\nDigit Span",
    "Non-verbal\nreasoning",
    "Verbal reasoning",
    "Spatial span std.",
    "WCST",
    "Reading\ncomprehension"
  )) |> 
  knitr::kable()
```
