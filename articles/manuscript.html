<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Reproducible manuscript • aphantasiaCognitiveClustering</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Montserrat-0.4.10/font.css" rel="stylesheet">
<link href="../deps/DM_Sans-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><script src="manuscript_files/libs/quarto-html/popper.min.js"></script><script src="manuscript_files/libs/quarto-html/tippy.umd.min.js"></script><link href="manuscript_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="manuscript_files/libs/quarto-html/light-border.css" rel="stylesheet">
<script src="manuscript_files/libs/quarto-contrib/glightbox/glightbox.min.js"></script><link href="manuscript_files/libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="manuscript_files/libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Reproducible manuscript">
<meta property="og:image" content="https://m-delem.github.io/aphantasiaCognitiveClustering/logo.png">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">aphantasiaCognitiveClustering</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/aphantasiaCognitiveClustering.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/manuscript.html">Reproducible manuscript</a></li>
    <li><a class="dropdown-item" href="../articles/analysis_report.html">Comprehensive data analysis report</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/m-delem/aphantasiaCognitiveClustering/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-quarto">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Reproducible manuscript</h1>




      <small class="dont-index">Source: <a href="https://github.com/m-delem/aphantasiaCognitiveClustering/blob/master/vignettes/manuscript.qmd" class="external-link"><code>vignettes/manuscript.qmd</code></a></small>
      <div class="d-none name"><code></code></div>
    </div>






    <section class="section level2"><h2 class="unnumbered" id="beyond-visual-imagery-unsupervised-clustering-reveals-spatial-and-verbal-cognitive-profiles-in-aphantasia-and-typical-imagery">Beyond visual imagery: Unsupervised clustering reveals spatial and verbal cognitive profiles in aphantasia and typical imagery<a class="anchor" aria-label="anchor" href="#beyond-visual-imagery-unsupervised-clustering-reveals-spatial-and-verbal-cognitive-profiles-in-aphantasia-and-typical-imagery"></a>
</h2>
<p><strong>Keywords</strong>: aphantasia, mental imagery, individual differences, cognitive profiles, reasoning, working memory, unsupervised clustering</p>
</section><section class="section level2" data-number="1"><h2 data-number="1" id="introduction">
<span class="header-section-number">1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Visual imagery, commonly referred to as “seeing with the mind’s eye”, designates the pseudo-perceptual visual experience of mental images in the absence of the corresponding external stimulus <span class="citation" data-cites="pearsonHumanImaginationCognitive2019">(<a href="#ref-pearsonHumanImaginationCognitive2019" role="doc-biblioref">Pearson, 2019</a>)</span>. There are large individual differences in visual imagery vividness (i.e. the intensity and detail of mental images) across a spectrum going from the absence of mental imagery, a phenomenon recently named “aphantasia” <span class="citation" data-cites="zemanLivesImageryCongenital2015">(<a href="#ref-zemanLivesImageryCongenital2015" role="doc-biblioref">Zeman et al., 2015</a>)</span>, to extremely vivid and perception-like imagery, named “hyperphantasia” <span class="citation" data-cites="zemanPhantasiaPsychologicalSignificance2020">(<a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">Zeman et al., 2020</a>)</span>. The introduction of the term “aphantasia” in 2015 led to a wave of research on the subject, exploring its underlying causes and consequences and potential positive or negative outcomes. A large body of research on aphantasia mainly identified potential deficits associated with it. Specifically, the condition has been associated with a reduction in autobiographical memory <span class="citation" data-cites="dawesCognitiveProfileMultisensory2020 dawesMemoriesBlindMind2022 miltonBehavioralNeuralSignatures2021 monzelHippocampaloccipitalConnectivityReflects2023">(<a href="#ref-dawesCognitiveProfileMultisensory2020" role="doc-biblioref">Dawes et al., 2020</a>, <a href="#ref-dawesMemoriesBlindMind2022" role="doc-biblioref">2022</a>; <a href="#ref-miltonBehavioralNeuralSignatures2021" role="doc-biblioref">Milton et al., 2021</a>; <a href="#ref-monzelHippocampaloccipitalConnectivityReflects2023" role="doc-biblioref">Monzel, Leelaarporn, et al., 2023</a>)</span>, lack of temporal and future imagination <span class="citation" data-cites="miltonBehavioralNeuralSignatures2021">(<a href="#ref-miltonBehavioralNeuralSignatures2021" role="doc-biblioref">Milton et al., 2021</a>)</span>, increased prosopagnosia <span class="citation" data-cites="miltonBehavioralNeuralSignatures2021 palermoCongenitalLackExtraordinary2022 zemanPhantasiaPsychologicalSignificance2020">(<a href="#ref-miltonBehavioralNeuralSignatures2021" role="doc-biblioref">Milton et al., 2021</a>; <a href="#ref-palermoCongenitalLackExtraordinary2022" role="doc-biblioref">Palermo et al., 2022</a>; <a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">Zeman et al., 2020</a>)</span>, reduced dreams <span class="citation" data-cites="dawesCognitiveProfileMultisensory2020">(<a href="#ref-dawesCognitiveProfileMultisensory2020" role="doc-biblioref">Dawes et al., 2020</a>)</span>, or decreased motor simulation <span class="citation" data-cites="dupontExplicitImplicitMotor2024">(<a href="#ref-dupontExplicitImplicitMotor2024" role="doc-biblioref">Dupont et al., 2024</a>)</span>. This focus on deficits has left the potential positive aspects of aphantasia largely unexplored, even though empirical evidence from recent studies has shown that individuals with aphantasia performed as well as those with “typical” visual imagery in various types of tasks presumed to require this ability, such as visual or visuo-spatial working memory <span class="citation" data-cites="keoghVisualWorkingMemory2021 pounderOnlyMinimalDifferences2022 reederNonvisualSpatialStrategies2024">(<a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-pounderOnlyMinimalDifferences2022" role="doc-biblioref">Pounder et al., 2022</a>; <a href="#ref-reederNonvisualSpatialStrategies2024" role="doc-biblioref">Reeder et al., 2024</a>)</span>. Research that hinted at advantages of aphantasia have focused mainly on emotional processing. Individuals with aphantasia have been shown to be less prone to sensory sensitivity <span class="citation" data-cites="danceWhatLinkMental2021">(<a href="#ref-danceWhatLinkMental2021" role="doc-biblioref">Dance et al., 2021</a>)</span>, less reactive to the reading of frightening scenarios <span class="citation" data-cites="wickenCriticalRoleMental2021">(<a href="#ref-wickenCriticalRoleMental2021" role="doc-biblioref">Wicken et al., 2021</a>)</span>, and less sensible to intrusive memories <span class="citation" data-cites="keoghFewerIntrusiveMemories2023">(<a href="#ref-keoghFewerIntrusiveMemories2023" role="doc-biblioref">Keogh et al., 2023</a>)</span> suggesting that aphantasia could help to reduce sensory overwhelm, and potentially protect against emotional overreaction.</p>
<p>Recently, Monzel et al. <span class="citation" data-cites="monzelAphantasiaFrameworkNeurodivergence2023">(<a href="#ref-monzelAphantasiaFrameworkNeurodivergence2023" role="doc-biblioref">2023</a>)</span> proposed that aphantasia should be understood within the framework of “neurodivergence” as a state representing atypical but functional cognitive processing, with advantages and disadvantages. The specifics of this “alternative thinking” and its advantages, however, remain to be understood. Interestingly, <span class="citation" data-cites="zemanPhantasiaPsychologicalSignificance2020">Zeman et al. (<a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">2020</a>)</span> found that individuals with aphantasia seemed more likely to work in STEM fields (Science, Technology, Engineering, and Mathematics), whereas hyperphantasics, at the other end of the spectrum, were more likely to work in art-related professions. Drawing on the patterns emerging from their large-scale exploratory survey, <span class="citation" data-cites="zemanPhantasiaPsychologicalSignificance2020">Zeman et al. (<a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">2020</a>)</span> proposed a broad hypothesis that, whereas hyperphantasia might be characterized by an episodic and sensorily-rich mode of thinking, aphantasia might be characterized by a more semantic and fact-oriented approach. The results and conclusions of <span class="citation" data-cites="zemanPhantasiaPsychologicalSignificance2020">Zeman et al. (<a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">2020</a>)</span> are very similar to the Object-Spatial-Verbal model of cognitive styles developed by <span class="citation" data-cites="blazhenkovaNewObjectspatialverbalCognitive2009">Blazhenkova &amp; Kozhevnikov (<a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">2009</a>)</span> and its associated questionnaire (Object-Spatial Imagery and Verbal Questionnaire, OSIVQ). Based on behavioural and neuroimaging studies on healthy individuals <span class="citation" data-cites="kosslynCognitiveNeuroscienceMental1995 wallaceImageryVividnessHypnotic1990 kozhevnikovRevisingVisualizerVerbalizerDimension2002">(<a href="#ref-kosslynCognitiveNeuroscienceMental1995" role="doc-biblioref">Kosslyn et al., 1995</a>; <a href="#ref-kozhevnikovRevisingVisualizerVerbalizerDimension2002" role="doc-biblioref">Kozhevnikov et al., 2002</a>; <a href="#ref-wallaceImageryVividnessHypnotic1990" role="doc-biblioref">Wallace, 1990</a>)</span> and neuropsychological studies of brain-damaged patients <span class="citation" data-cites="farahCaseStudyMental1988 bartolomeoRelationshipVisualPerception2002">(<a href="#ref-bartolomeoRelationshipVisualPerception2002" role="doc-biblioref">Bartolomeo, 2002</a>; <a href="#ref-farahCaseStudyMental1988" role="doc-biblioref">Farah et al., 1988</a>)</span>, <span class="citation" data-cites="blazhenkovaObjectSpatialImagery2006">Blazhenkova et al. (<a href="#ref-blazhenkovaObjectSpatialImagery2006" role="doc-biblioref">2006</a>)</span> showed that visual-object imagery (imagery for colors, shapes, brightness, etc.) could be dissociated from spatial imagery (imagery for location, movement and orientation). <span class="citation" data-cites="blazhenkovaNewObjectspatialverbalCognitive2009">Blazhenkova &amp; Kozhevnikov (<a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">2009</a>)</span> challenged the prevailing Visualizer-Verbalizer model of cognitive styles <span class="citation" data-cites="paivioImageryAbilityVisual1971 richardsonMeaningMeasurementMemory1977">(<a href="#ref-paivioImageryAbilityVisual1971" role="doc-biblioref">Paivio &amp; Ernest, 1971</a>; <a href="#ref-richardsonMeaningMeasurementMemory1977" role="doc-biblioref">Richardson, 1977</a>)</span> to introduce the spatial dimension as a major form of imagery and cognitive style in its own right, alongside visual and verbal styles. They showed that several widely used paradigms, such as the Mental Rotation Task <span class="citation" data-cites="shepardMentalRotationThreeDimensional1971">(<a href="#ref-shepardMentalRotationThreeDimensional1971" role="doc-biblioref">Shepard &amp; Metzler, 1971</a>)</span> or Paper Folding Test <span class="citation" data-cites="ekstromKitFactorreferencedCognitive1976">(<a href="#ref-ekstromKitFactorreferencedCognitive1976" role="doc-biblioref">Ekstrom, 1976</a>)</span>, often considered visual, were not associated with visual imagery or visual cognitive styles <em>per se</em>, but with spatial imagery and spatial cognitive styles <span class="citation" data-cites="blazhenkovaObjectSpatialImagery2006 blazhenkovaNewObjectspatialverbalCognitive2009 kozhevnikovTradeoffObjectSpatial2010 vannucciIndividualDifferencesVisuospatial2006">(<a href="#ref-blazhenkovaObjectSpatialImagery2006" role="doc-biblioref">Blazhenkova et al., 2006</a>; <a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">Blazhenkova &amp; Kozhevnikov, 2009</a>; <a href="#ref-kozhevnikovTradeoffObjectSpatial2010" role="doc-biblioref">Kozhevnikov et al., 2010</a>; <a href="#ref-vannucciIndividualDifferencesVisuospatial2006" role="doc-biblioref">Vannucci et al., 2006</a>)</span>. Consistent with the observation of <span class="citation" data-cites="zemanPhantasiaPsychologicalSignificance2020">Zeman et al. (<a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">2020</a>)</span> of a prevalence of STEM occupations in aphantasia and artists in hyperphantasia, several studies on the Object-Spatial-Verbal model have shown visual-object cognitive styles to be particularly prevalent among visual artists, while spatial styles are over-represented in scientific fields and verbal styles prevail in literature and the humanities, both among students and professionals <span class="citation" data-cites="blazhenkovaObjectSpatialImagery2006 blazhenkovaNewObjectspatialverbalCognitive2009 blazhenkovaVisualobjectAbilityNew2010 kozhevnikovSpatialObjectVisualizers2005 kozhevnikovTradeoffObjectSpatial2010">(<a href="#ref-blazhenkovaObjectSpatialImagery2006" role="doc-biblioref">Blazhenkova et al., 2006</a>; <a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">Blazhenkova &amp; Kozhevnikov, 2009</a>, <a href="#ref-blazhenkovaVisualobjectAbilityNew2010" role="doc-biblioref">2010</a>; <a href="#ref-kozhevnikovSpatialObjectVisualizers2005" role="doc-biblioref">Kozhevnikov et al., 2005</a>; <a href="#ref-kozhevnikovTradeoffObjectSpatial2010" role="doc-biblioref">Kozhevnikov et al., 2010</a>)</span>. These results are corroborated by various studies showing that spatial imagery is preserved or enhanced in aphantasia, both on the subjective spatial scale of the Object-Spatial Imagery Questionnaire <span class="citation" data-cites="blazhenkovaObjectSpatialImagery2006">(OSIQ, the first version of the OSIVQ without the verbal scale, <a href="#ref-blazhenkovaObjectSpatialImagery2006" role="doc-biblioref">Blazhenkova et al., 2006</a>)</span> and on various spatial rotation, manipulation or spatial working memory tasks <span class="citation" data-cites="zemanLivesImageryCongenital2015 bainbridgeQuantifyingAphantasiaDrawing2021 dawesCognitiveProfileMultisensory2020 keoghBlindMindNo2018 keoghVisualWorkingMemory2021 pounderOnlyMinimalDifferences2022 reederNonvisualSpatialStrategies2024">(<a href="#ref-bainbridgeQuantifyingAphantasiaDrawing2021" role="doc-biblioref">Bainbridge et al., 2021</a>; <a href="#ref-dawesCognitiveProfileMultisensory2020" role="doc-biblioref">Dawes et al., 2020</a>; <a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-keoghBlindMindNo2018" role="doc-biblioref">Keogh &amp; Pearson, 2018</a>; <a href="#ref-pounderOnlyMinimalDifferences2022" role="doc-biblioref">Pounder et al., 2022</a>; <a href="#ref-reederNonvisualSpatialStrategies2024" role="doc-biblioref">Reeder et al., 2024</a>; <a href="#ref-zemanLivesImageryCongenital2015" role="doc-biblioref">Zeman et al., 2015</a>)</span>.</p>
<p>Several studies have also revealed a wide range of spatial, sensorimotor/kinaesthetic, verbal or amodal memory strategies reported by individuals with aphantasia in (supposedly) visual tasks <span class="citation" data-cites="keoghVisualWorkingMemory2021 reederNonvisualSpatialStrategies2024 zemanPhantasiaPsychologicalSignificance2020">(<a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-reederNonvisualSpatialStrategies2024" role="doc-biblioref">Reeder et al., 2024</a>; <a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">Zeman et al., 2020</a>)</span>. The diversity in modes of thinking could therefore be distributed across several dimensions, including visual-object or spatial representation, but potentially extending to verbal and semantic domains. The verbal (or “propositional”) aspect of representations and cognitive strategies, although often mentioned as a potential candidate for alternative strategies in visual aphantasia, has scarcely been studied. The Object-Spatial-Verbal model of cognitive styles could allow to study verbal representations in a coherent framework alongside visual and spatial imagery and shed light on the cognitive strategies of individuals with aphantasia. Therefore, the objective of the present study was two-fold: (a) to explore the cognitive profiles of individuals with aphantasia using the Object-Spatial-Verbal model of imagery and cognitive styles theorised by <span class="citation" data-cites="blazhenkovaNewObjectspatialverbalCognitive2009">Blazhenkova &amp; Kozhevnikov (<a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">2009</a>)</span>, and (b) to explore whether the profiles might be related to cognitive performance.</p>
<p>We hypothesised that individuals with aphantasia would tend to adopt spatial and verbal cognitive profiles, and that these profiles would be associated with specific performance patterns. First, we hypothesised that the profiles might be related to reasoning performance. Spatial imagery is known to be involved and to play a major role in abstract reasoning <span class="citation" data-cites="waiSpatialAbilitySTEM2009">(<a href="#ref-waiSpatialAbilitySTEM2009" role="doc-biblioref">Wai et al., 2009</a>)</span>. In this context, the absence of visual imagery in aphantasia and the priority and focus on spatial representations in aphantasia <span class="citation" data-cites="bainbridgeQuantifyingAphantasiaDrawing2021 keoghVisualWorkingMemory2021 reederNonvisualSpatialStrategies2024">(<a href="#ref-bainbridgeQuantifyingAphantasiaDrawing2021" role="doc-biblioref">Bainbridge et al., 2021</a>; <a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-reederNonvisualSpatialStrategies2024" role="doc-biblioref">Reeder et al., 2024</a>)</span> could be hypothesised to facilitate abstract reasoning. Second, we hypothesised that spatial or verbal cognitive profiles could explain individual differences in working memory performance, depending on the modality involved. Previous studies failed to find differences in working memory performance between individuals with aphantasia and controls <span class="citation" data-cites="keoghVisualWorkingMemory2021 pounderOnlyMinimalDifferences2022 reederNonvisualSpatialStrategies2024">(e.g., <a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-pounderOnlyMinimalDifferences2022" role="doc-biblioref">Pounder et al., 2022</a>; <a href="#ref-reederNonvisualSpatialStrategies2024" role="doc-biblioref">Reeder et al., 2024</a>)</span>, but only took into account variations on the visual-object dimension of imagery. Accounting for the use of spatial and verbal representations in working memory could provide insight into the processes and strategies underlying the performance of individuals with aphantasia in various tasks <span class="citation" data-cites="pearsonRedefiningVisualWorking2019">(<a href="#ref-pearsonRedefiningVisualWorking2019" role="doc-biblioref">Pearson &amp; Keogh, 2019</a>)</span>. Third, we put forward the very general hypothesis that if individuals with aphantasia have distinct verbal cognitive profiles, they should have very good reading comprehension skills. However, some studies have established a positive correlation between reading comprehension and visual mental imagery <span class="citation" data-cites="suggateMentalImagerySkill2022">(e.g., <a href="#ref-suggateMentalImagerySkill2022" role="doc-biblioref">Suggate &amp; Lenhard, 2022</a>)</span>, suggesting a central role for the latter. To date no specific study of reading comprehension in an ecological context in aphantasia could provide a definitive answer as to the advantages or disadvantages of aphantasia in complex reading tasks involving verbal skills, working memory and mental imagery. Finally, we hypothesised that the performance of individuals with aphantasia in tasks supposed to require visual imagery might be linked to a greater flexibility in switching to alternative strategies (e.g. propositional, motor, etc.). In this case, they should be characterised by particularly efficient executive functioning. Thus, the present study also included a task designed to probe executive functions.</p>
<p>In sum, the present study aimed to gain a better understanding of the cognitive profiles of individuals with aphantasia and their strategies for representing and processing information. We sought to identify patterns of performance in accomplishing various cognitive tasks from individuals with aphantasia and controls and relate these to their subjective preferences for visual, spatial or verbal processing. To this end, an online study was designed, integrating questionnaires and behavioural tasks to assess visual imagery, spatial imagery, verbal strategies, spatial, verbal and non-verbal reasoning, verbal and visuo-spatial working memory, reading comprehension, and executive functions. Based on previous work showing very few differences in cognitive performance between individuals with aphantasia and controls <span class="citation" data-cites="keoghVisualWorkingMemory2021 pounderOnlyMinimalDifferences2022 knightMemoryImageryNo2022">(<a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-knightMemoryImageryNo2022" role="doc-biblioref">Knight et al., 2022</a>; <a href="#ref-pounderOnlyMinimalDifferences2022" role="doc-biblioref">Pounder et al., 2022</a>)</span>, we expected that dividing the participants into two groups according to visual imagery ability would not fully explain substantial differences in task performance. Thus, we planned to explore the hypothesis of hidden sub-groups within the sample using a data-driven unsupervised clustering method. This analysis included all measures of cognitive abilities to assess similarities and differences between participants. The proposed data analysis plan resulted in clusters characterized by their visual, spatial, and verbal cognitive styles, which were able to explain task performance. In the light of these patterns, we then discuss the potential of the Object-Spatial-Verbal model for understanding cognitive processes and strategies in aphantasia.</p>
</section><section class="section level2" data-number="2"><h2 data-number="2" id="methods">
<span class="header-section-number">2</span> Methods<a class="anchor" aria-label="anchor" href="#methods"></a>
</h2>
<p>No part of the study procedures or analysis plan was preregistered prior to the research being undertaken. We report all data exclusions, all inclusion/exclusion criteria, all manipulations, and all measures in the study.</p>
<section class="level2" data-number="2.1"><h3 data-number="2.1" id="participants">
<span class="header-section-number">2.1</span> Participants<a class="anchor" aria-label="anchor" href="#participants"></a>
</h3>
<p>Participants had to be French speakers, had normal or corrected vision and none of the participants reported to have any known reading disorders. They were recruited online both on groups unrelated to mental imagery (social networks, French cognitive science information network, etc.) and on groups dedicated to aphantasia and visual imagery. The study link was sent to participants who volunteered by contacting the team by email. All procedures performed in these experiments were in accordance with the ethical standards of the institutional research committee and with the Helsinki declaration and its later amendments or comparable ethical standards. Informed consent was obtained from all individual participants included in the study . Participation was without compensation. As the study was exploratory, the sample size was not determined <em>a priori</em>. Only data from participants who completed all the tasks were included in the analyses. Of the 1200 people who opened the link to the study, 96 completed all the tasks, making up the final sample. The questionnaire statistics are detailed in the results part below.</p>
</section><section class="level2" data-number="2.2"><h3 data-number="2.2" id="materials">
<span class="header-section-number">2.2</span> Materials<a class="anchor" aria-label="anchor" href="#materials"></a>
</h3>
<section class="level3" data-number="2.2.1"><h4 data-number="2.2.1" id="questionnaires">
<span class="header-section-number">2.2.1</span> Questionnaires<a class="anchor" aria-label="anchor" href="#questionnaires"></a>
</h4>
<p>The Vividness of Visual Imagery Questionnaire <span class="citation" data-cites="marksVisualImageryDifferences1973">(VVIQ, <a href="#ref-marksVisualImageryDifferences1973" role="doc-biblioref">Marks, 1973</a>)</span> was used to assess visual imagery ability. The VVIQ is a 16-item self-report scale that asks participants to imagine a person and several scenes and to rate the vividness of these mental images using a 5-point scale ranging from 1 (“<em>No imagery at all, you just know you’re thinking about the object</em>”) to 5 (“<em>Perfectly clear and as vivid as normal vision</em>”). Scores range from 16 to 80. The total score of 32, conventionally used as a threshold to define aphantasia, is equivalent to a score of 2 (“<em>vague and faint</em>”) for each item in the questionnaire. The internal reliability (Cronbach’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>) of the VVIQ is .88 <span class="citation" data-cites="mckelvieVVIQPsychometricTest1995">(<a href="#ref-mckelvieVVIQPsychometricTest1995" role="doc-biblioref">McKelvie, 1995</a>)</span>.</p>
<p>The Object-Spatial Imagery and Verbal Questionnaire <span class="citation" data-cites="blazhenkovaNewObjectspatialverbalCognitive2009">(OSIVQ, <a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">Blazhenkova &amp; Kozhevnikov, 2009</a>)</span> was used to evaluate imagery strategies and cognitive styles. The OSIVQ is a 45-item scale that asks participants to indicate the extent to which each of the statements applied to them, about visual-object imagery ability (e.g., “<em>When I imagine a friend’s face, I have a perfectly clear and bright image</em>”), visuo-spatial imagery ability (e.g., “<em>My images tend to be schematic representations of things and events rather than detailed images</em>”) or verbal strategies for processing information (e.g., “<em>When I remember a scene, I use verbal descriptions rather than mental images</em>”), on a 5-point scale ranging from 1 (“<em>Totally disagree</em>”) to 5 (“<em>Totally agree</em>”). Each sub-scale (object, spatial, verbal) comprises 15 items whose values are added together to obtain a score ranging from 15 to 75. Cronbach’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> of the object, spatial and verbal scales are .83, .79 and .74 respectively <span class="citation" data-cites="blazhenkovaNewObjectspatialverbalCognitive2009">(<a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">Blazhenkova &amp; Kozhevnikov, 2009</a>)</span>.</p>
<p>As mental imagery is a multi-sensory experience that is not limited to vision, the Plymouth Sensory Imagery Questionnaire <span class="citation" data-cites="andradeAssessingVividnessMental2014">(Psi-Q, <a href="#ref-andradeAssessingVividnessMental2014" role="doc-biblioref">Andrade et al., 2014</a>)</span> was used to assess imagery vividness across various sensory modalities. The Psi-Q (in its short form) comprises seven sets of three items for each of the following modalities: <em>Vision</em>, <em>Hearing</em>, <em>Smell</em>, <em>Taste</em>, <em>Touch</em>, <em>Bodily Sensation</em> and <em>Emotional Feeling</em>. Each set has a heading such as “<em>Imagine the appearance of…</em>”} and then three items. Participants were asked to rate their image on an 11-point scale anchored by 0 (“<em>No image at all</em>”) and 10 (“<em>As vivid as real life</em>”), thus yielding scores ranging from 0 to 33 for each modality. Cronbach’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> of the 21-item Psi-Q is .91 <span class="citation" data-cites="andradeAssessingVividnessMental2014">(<a href="#ref-andradeAssessingVividnessMental2014" role="doc-biblioref">Andrade et al., 2014</a>)</span>.</p>
</section><section class="level3" data-number="2.2.2"><h4 data-number="2.2.2" id="tasks">
<span class="header-section-number">2.2.2</span> Tasks<a class="anchor" aria-label="anchor" href="#tasks"></a>
</h4>
<p>The Raven Standard Progressive Matrices <span class="citation" data-cites="ravenRavenProgressiveMatrices1938">(hereinafter called Raven matrices, <a href="#ref-ravenRavenProgressiveMatrices1938" role="doc-biblioref">Raven &amp; Court, 1938</a>)</span> is a widely-used assessment to estimate fluid intelligence (non-verbal visual perception ability) and abstract reasoning (analogical and deductive reasoning abilities). The Raven matrices contains 60 items divided into five sets. Each question consists in completing a missing figure in a matrix of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math> figures by extracting and following the logical rules underlying the organisation of the matrices. Items are of increasing difficulty. At the end of a set, the difficulty decreases again but the logical rule changes, and the successive sets have increasing difficulty. A shortened clinical version with two short forms of 9 items developed by <span class="citation" data-cites="bilkerDevelopmentAbbreviatedNineItem2012">Bilker et al. (<a href="#ref-bilkerDevelopmentAbbreviatedNineItem2012" role="doc-biblioref">2012</a>)</span> was used, predicting the 60-item score with good accuracy. Each of the short forms had correlations of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>.98</mn></mrow><annotation encoding="application/x-tex">r = .98</annotation></semantics></math> with the long form, and respective Cronbach’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> of .80 and .83. This shortened version represents a 70% reduction in the number of items to be administered and in test-taking time, for psychometric characteristics similar to those of the full form <span class="citation" data-cites="bilkerDevelopmentAbbreviatedNineItem2012">(<a href="#ref-bilkerDevelopmentAbbreviatedNineItem2012" role="doc-biblioref">Bilker et al., 2012</a>)</span>.</p>
<p>The Spatial Reasoning Instrument <span class="citation" data-cites="ramfulMeasurementSpatialAbility2017">(SRI, <a href="#ref-ramfulMeasurementSpatialAbility2017" role="doc-biblioref">Ramful et al., 2017</a>)</span> is a 30-item test for measuring spatial ability along three constructs: mental rotation, spatial orientation, and spatial visualisation. The test showed good validity an psychometric properties in three areas: (a) the exploratory factor analysis of the subscales (mental rotation, spatial orientation, spatial visualisation), (b) the Rasch analysis of item reliability within each construct, and (c) the significant correlations (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>.33</mn><mo>,</mo><mn>.62</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">r \in [.33, .62]</annotation></semantics></math>) and person separation reliability with four existing well-regarded instruments measuring spatial reasoning, the Card Rotation Test, the Cube Comparison Test, the Paper Folding Test [all from <span class="citation" data-cites="ekstromKitFactorreferencedCognitive1976">Ekstrom (<a href="#ref-ekstromKitFactorreferencedCognitive1976" role="doc-biblioref">1976</a>)</span>] and the Perspective Taking (Spatial Orientation) Test <span class="citation" data-cites="kozhevnikovDissociationObjectManipulation2001">(<a href="#ref-kozhevnikovDissociationObjectManipulation2001" role="doc-biblioref">Kozhevnikov &amp; Hegarty, 2001</a>)</span>. The internal reliability (Cronbach’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>) of the SRI is .85.</p>
<p>The Similarities sub-test of the Weschler Adult Intelligence Scale <span class="citation" data-cites="wechslerWechslerAdultIntelligence2008">(WAIS-IV, <a href="#ref-wechslerWechslerAdultIntelligence2008" role="doc-biblioref">Wechsler et al., 2008</a>)</span> is a well-known assessment to estimate verbal comprehension abilities. Specifically, this test assesses both verbal concept formation and verbal abstract reasoning. It comprises 18 pairs of words in which the participant has to identify the underlying qualitative relationship (e.g., “<em>How are DREAM and REALITY similar?</em>”). Accurate answers (rated according to a standardized response scale) receive two points, approximate answers one point, and vague or incorrect answers zero, giving a maximum score of 36. After three zero scores, the task stops. Due to the internet-based nature of the experiment, all participants passed all the items, but only the <em>scoring</em> of their answers stopped after three incorrect answers. The scoring was carried out manually, using double scoring by the first two authors of this article, blind to the groups and participants. All participants performed above the fifth percentile on the Similarities WAIS-IV sub-test (score <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>≥</mo><annotation encoding="application/x-tex">\geq</annotation></semantics></math> 12/36), thereby confirming that none of the participants presented a deficit in semantic oral language skills.</p>
<p>Reverse Corsi blocks is a spatial memory span task <span class="citation" data-cites="gibeauCorsiBlocksTask2021">(<a href="#ref-gibeauCorsiBlocksTask2021" role="doc-biblioref">Gibeau, 2021</a>)</span> assessing visuo-spatial working memory. The task consists in presenting the participant with a grid of blocks in a frame, then displaying a sequence of blocks (the blocks changing colours in turn), at a rate of one per second, and asking the participant to recall it in reverse order, from the last block to the first. The task began with a short sequence of three blocks, increasing with each success or decreasing after two failures, over a fixed total of 14 trials. The average number of blocks recalled correctly at the correct position was retained as the score for the task.</p>
<p>The reverse digit span is a verbal memory span task assessing verbal working memory <span class="citation" data-cites="blackburnRevisedAdministrationScoring1957">(<a href="#ref-blackburnRevisedAdministrationScoring1957" role="doc-biblioref">Blackburn &amp; Benton, 1957</a>)</span>. The task involves presenting numbers at a rate of 1 per second, and then asking the participant to recall them from the last to the first. The task begins with a short sequence of three digits, then lengthens with each success or decreases after two failures, over a total of 14 trials. The average number of digits recalled correctly at the correct position was retained as the score for the task.</p>
<p>The Wisconsin Card Sorting Test <span class="citation" data-cites="heatonWisconsinCardSorting1993">(WCST, <a href="#ref-heatonWisconsinCardSorting1993" role="doc-biblioref">Heaton &amp; Staff, 1993</a>)</span> is a widely used test of set-shifting ability which was developed to measure flexibility of human thought and the ability to shift cognitive strategy in response to changing contingencies. The WCST is designed to measure executive functioning including attentional set shifting, task/rule switching or reversal, and working memory abilities. The assessment requires the participants to sort 64 cards according to color (red, blue, yellow or green), shape (cross, circle, triangle or star) or number of figures (one, two, three or four). Over the course of the task, the sorting rule discreetly changes from color to shape or number of figures, without the participants being informed. Participants must modify their predictions and choices accordingly and sort the cards according to the new sorting rule: they receive feedback for their response (correct or incorrect), which should enable them to improve with implicit rule extraction <span class="citation" data-cites="nelsonModifiedCardSorting1976">(<a href="#ref-nelsonModifiedCardSorting1976" role="doc-biblioref">Nelson, 1976</a>)</span>. The final score used was the percentage of correct sorts after 64 trials. The WCST, scored according to the percentage (or number) of correct sorts, exhibits satisfying split-half reliability <span class="citation" data-cites="koppReliabilityWisconsinCard2021">(Spearman-Brown <em>r</em> = .90, <a href="#ref-koppReliabilityWisconsinCard2021" role="doc-biblioref">Kopp et al., 2021</a>)</span>.</p>
<p>The reading comprehension task assesses both explicit literal comprehension and inferential comprehension skills and was designed for assessment in adult readers <span class="citation" data-cites="brethesTextReadingFluency2022">(<a href="#ref-brethesTextReadingFluency2022" role="doc-biblioref">Brèthes et al., 2022</a>)</span>. Reading comprehension is a complex cognitive activity that involves a number of skills including word recognition skills, grammar, semantic and general knowledge, working memory and reasoning skills as well as inference-making abilities. This reading comprehension task was composed of three texts drawn from the French daily newspaper <em>Le Monde</em>, all dealing with the destruction of the Great Barrier Reef and its various causes. The participant had to read each text without time constraints, then answer eight questions: four questions about explicit literal comprehension and four inferential questions about the comprehension of the implicit information in the texts, among which two examined text-connecting inferences and two examined knowledge-based inferences. While text-connecting inference skills required participants to integrate text information in order to establish local cohesiveness, knowledge-based inference skills required participants to establish links between the text content and their own personal knowledge. The questions were also divided between two question formats: open questions and multiple-choice questions. Participants were not allowed to refer to the text when answering the questions. In all, the test contained 20 questions whose answers were scored by the experimenter, with 2 points for complete answers, 1 point for incomplete answers and 0 for incorrect answers. The maximum score was therefore 40 points. The scoring was carried out manually, using double scoring by the first two authors of the present article, blind to the groups and participants. Cronbach’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> of the task is 0.78.</p>
</section></section><section class="level2" data-number="2.3"><h3 data-number="2.3" id="experimental-design-and-procedure">
<span class="header-section-number">2.3</span> Experimental design and procedure<a class="anchor" aria-label="anchor" href="#experimental-design-and-procedure"></a>
</h3>
<p>The experiment was administered online via a JATOS server <span class="citation" data-cites="langeJustAnotherTool2015">(<a href="#ref-langeJustAnotherTool2015" role="doc-biblioref">Lange et al., 2015</a>)</span>. It was programmed using SurveyJS and jsPsych <span class="citation" data-cites="leeuwJsPsychEnablingOpenSource2023">(<a href="#ref-leeuwJsPsychEnablingOpenSource2023" role="doc-biblioref">Leeuw et al., 2023</a>)</span>, open-source JavaScript libraries dedicated to the creation of questionnaires and experiments respectively, as well as OpenSesame <span class="citation" data-cites="mathotOpenSesameOpensourceGraphical2012">(<a href="#ref-mathotOpenSesameOpensourceGraphical2012" role="doc-biblioref">Mathôt et al., 2012</a>)</span>, a graphical interface for the construction of behavioural experiments. The link to the experiment was emailed individually to each volunteer participant and could only be used once per participant.</p>
<p>All participants were subjected to the same study design and task sequence. Before the first questionnaires, participants gave their consent, then demographic data were collected (first language, age, gender, occupation, education and field of study, vision). Due to the length of the protocol (Median time spent = 84.58 min, Median Absolute Deviation = 27.51 min), the experiment was structured with instructions accompanied by pages of explanations to reinforce engagement and focus (e.g., inviting people to “dive into their minds” or “test their abilities”). No text mentioned the word aphantasia, to avoid the stigma, bias or preconceived ideas specifically associated with this term <span class="citation" data-cites="cabbaiInvestigatingRelationshipsTrait2023 monzelAphantasiaFrameworkNeurodivergence2023">(see <a href="#ref-cabbaiInvestigatingRelationshipsTrait2023" role="doc-biblioref">Cabbai et al., 2023</a>; <a href="#ref-monzelAphantasiaFrameworkNeurodivergence2023" role="doc-biblioref">Monzel, Dance, et al., 2023</a>)</span>. The experiment started with the VVIQ, followed by the Raven matrices, the WCST, the OSIVQ, the SRI, the reverse Corsi blocks, the Similarities test, the Reading comprehension task, the reverse digit span, and ended with the Psi-Q. None of the tasks had a time limit, but participants were instructed to respond as soon as they had the answer while maintaining accuracy, with the aim of reducing the total duration of the experiment for them. However, given that the experiment was long, that the participants were not monitored due to the online format, and that the instructions did not place particular emphasis on speed of response as a key aspect, response times were not analysed.</p>
</section><section class="level2" data-number="2.4"><h3 data-number="2.4" id="analyses">
<span class="header-section-number">2.4</span> Analyses<a class="anchor" aria-label="anchor" href="#analyses"></a>
</h3>
<p>All analyses were conducted using the R statistical language <span class="citation" data-cites="R-base">(<a href="#ref-R-base" role="doc-biblioref">R Core Team, 2024</a>)</span> on RStudio <span class="citation" data-cites="Rstudio">(<a href="#ref-Rstudio" role="doc-biblioref">Posit team, 2025</a>)</span>. Data curation was handled in R with packages from the <em>tidyverse</em> collection <span class="citation" data-cites="tidyverse2019">(<a href="#ref-tidyverse2019" role="doc-biblioref">Wickham et al., 2019</a>)</span>. All visualisations were produced with the packages <em>ggplot2</em> <span class="citation" data-cites="ggplot22016">(<a href="#ref-ggplot22016" role="doc-biblioref">Wickham, 2016</a>)</span>, <em>factoextra</em> <span class="citation" data-cites="R-factoextra">(<a href="#ref-R-factoextra" role="doc-biblioref">Kassambara &amp; Mundt, 2020</a>)</span>, <em>see</em> <span class="citation" data-cites="see2021">(<a href="#ref-see2021" role="doc-biblioref">Lüdecke et al., 2021</a>)</span>, <em>superb</em> <span class="citation" data-cites="superb2021">(<a href="#ref-superb2021" role="doc-biblioref">Cousineau et al., 2021</a>)</span> and <em>patchwork</em> <span class="citation" data-cites="R-patchwork">(<a href="#ref-R-patchwork" role="doc-biblioref">Pedersen, 2024</a>)</span>.</p>
<p>Bayesian modelling used throughout the analyses was conducted using the R packages <em>rstanarm</em> <span class="citation" data-cites="R-rstanarm">(<a href="#ref-R-rstanarm" role="doc-biblioref">Gabry &amp; Goodrich, 2024</a>)</span>, <em>BayesFactor</em> <span class="citation" data-cites="R-BayesFactor">(<a href="#ref-R-BayesFactor" role="doc-biblioref">Morey &amp; Rouder, 2024</a>)</span> and <em>bayestestR</em> <span class="citation" data-cites="bayestestR2019">(<a href="#ref-bayestestR2019" role="doc-biblioref">Makowski et al., 2019</a>)</span>, and unless otherwise stated default parameter values were used. The R package <em>emmeans</em> <span class="citation" data-cites="R-emmeans">(<a href="#ref-R-emmeans" role="doc-biblioref">Lenth, 2024</a>)</span> was used for marginal estimates and contrast analyses. For all tests, the statistic reported, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math>, quantifies the relative “weight of evidence” in favour of the hypothesis <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_{1}</annotation></semantics></math> (e.g., the effect of a factor), against the null hypothesis <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_{0}</annotation></semantics></math><span class="citation" data-cites="goodWeightEvidenceBrief1985">(<a href="#ref-goodWeightEvidenceBrief1985" role="doc-biblioref">Good, 1985</a>)</span>. According to Jeffrey’s scale thresholds <span class="citation" data-cites="kassBayesFactors1995">(see <a href="#ref-kassBayesFactors1995" role="doc-biblioref">Kass &amp; Raftery, 1995</a>)</span>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>;</mo><mn>0.5</mn><mo stretchy="false" form="prefix">[</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \in [0;0.5[</annotation></semantics></math> = “<em>Barely worth mentioning</em>”; <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∈</mo><mo stretchy="false" form="prefix">[</mo><mn>0.5</mn><mo>;</mo><mn>1</mn><mo stretchy="false" form="prefix">[</mo></mrow><annotation encoding="application/x-tex">\in [0.5;1[</annotation></semantics></math> = “<em>Substantial evidence</em>”; <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∈</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>;</mo><mn>2</mn><mo stretchy="false" form="prefix">[</mo></mrow><annotation encoding="application/x-tex">\in [1;2[</annotation></semantics></math> = “<em>Strong evidence</em>”; <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∈</mo><mo stretchy="false" form="prefix">[</mo><mn>2</mn><mo>;</mo><mi>+</mi><mi>∞</mi><mo stretchy="false" form="prefix">[</mo></mrow><annotation encoding="application/x-tex">\in [2;+\infty[</annotation></semantics></math> = “<em>Decisive evidence</em>”. The same negative thresholds apply for the weight of evidence in favour of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>.</p>
</section></section><section class="section level2" data-number="3"><h2 data-number="3" id="results">
<span class="header-section-number">3</span> Results<a class="anchor" aria-label="anchor" href="#results"></a>
</h2>
<p>The final sample comprised 96 participants (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">M_{age}</annotation></semantics></math> = 32.5, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SD_{age}</annotation></semantics></math> = 11.3, range<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">_{age}</annotation></semantics></math> = [19, 65], 74 females, 21 males, 1 another gender). The most widely used criterion in studies on aphantasia to identify the condition is a score inferior to 32 on the Vividness of Visual Imagery Questionnaire <span class="citation" data-cites="marksVisualImageryDifferences1973">(<a href="#ref-marksVisualImageryDifferences1973" role="doc-biblioref">Marks, 1973</a>)</span>. After applying this criterion, we identified 45 individuals with aphantasia in the sample (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mrow><mi>V</mi><mi>V</mi><mi>I</mi><mi>Q</mi></mrow></msub><annotation encoding="application/x-tex">M_{VVIQ}</annotation></semantics></math> = 18.3, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mrow><mi>V</mi><mi>V</mi><mi>I</mi><mi>Q</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SD_{VVIQ}</annotation></semantics></math> = 4.2, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">M_{age}</annotation></semantics></math> = 33.8, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SD_{age}</annotation></semantics></math> = 10.6, 37 females, 8 males) and 51 controls (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mrow><mi>V</mi><mi>V</mi><mi>I</mi><mi>Q</mi></mrow></msub><annotation encoding="application/x-tex">M_{VVIQ}</annotation></semantics></math> = 58.1, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mrow><mi>V</mi><mi>V</mi><mi>I</mi><mi>Q</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SD_{VVIQ}</annotation></semantics></math> = 10.8, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub><annotation encoding="application/x-tex">M_{age}</annotation></semantics></math> = 31.4, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mrow><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SD_{age}</annotation></semantics></math> = 11.8, 37 females, 13 males, 1 another gender).</p>
<section class="level2" data-number="3.1"><h3 data-number="3.1" id="vviq-groups-analysis">
<span class="header-section-number">3.1</span> VVIQ groups analysis<a class="anchor" aria-label="anchor" href="#vviq-groups-analysis"></a>
</h3>
<section class="level3" data-number="3.1.1"><h4 data-number="3.1.1" id="education-and-occupation">
<span class="header-section-number">3.1.1</span> Education and occupation<a class="anchor" aria-label="anchor" href="#education-and-occupation"></a>
</h4>
<p>Participants’ level of education, field of study, and occupation were analysed to detect any association with the grouping factor. Levels of education were coded using the equivalent of the French levels in the International Standard Classification of Education (ISCED), i.e., <em>Upper secondary</em>, <em>Post-secondary</em>, <em>Bachelor</em>, <em>Master</em>, and <em>Doctorate</em>. Fields of study have been coded according to the 10 broad categories defined by the ISCED-F 2013 (ISCED: Fields of Education and Training). Occupations have been coded according to the sub-major groups of the International Standard Classification of Occupations (ISCO-08) for an appropriate level of precision given our sample size. Nine occupational groups were identified in the sample. Bayes factors for independence were calculated to evaluate the association between groups and each demographic variable <span class="citation" data-cites="gunelBayesFactorsIndependence1974">(see <a href="#ref-gunelBayesFactorsIndependence1974" role="doc-biblioref">Gûnel &amp; Dickey, 1974</a>)</span>. The tests found evidence against a relationship between groups and levels of education (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -4.88), groups and fields of study (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -5.41), or groups and occupation (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -4.37).</p>
</section><section class="level3" data-number="3.1.2"><h4 data-number="3.1.2" id="questionnaire-and-task-results">
<span class="header-section-number">3.1.2</span> Questionnaire and task results<a class="anchor" aria-label="anchor" href="#questionnaire-and-task-results"></a>
</h4>
<p>The measured variables were the scores on the VVIQ, the three OSIVQ scales, the seven Psi-Q scales, the Raven matrices, the SRI, the Similarities Test, the reverse spatial and verbal spans, the WCST and the Reading comprehension task. Linear models have been fitted to the various variables to model them with participant groups as categorical predictors and age as a continuous covariate so as to control for the potential influence of the latter. Contrast analyses were thereafter conducted to assess the differences between the groups. All score differences (hereinafter referred to as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>) and their 95% Credible Intervals are reported in <a href="#tbl-g-results" class="quarto-xref">Table 1</a> along with the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> quantifying the weight of evidence in favour of a non-null difference between the groups.</p>
<p>Individuals with aphantasia had lower scores than controls in all visual imagery scales (VVIQ: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 64.33; OSIVQ-Object: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 37.94; Psi-Q Visual: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 58.94), but also on all other sensory imaging modalities evaluated by the Psi-Q (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>14</mn><mo>;</mo><mn>31</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10}) \in [14; 31]</annotation></semantics></math> for all modalities). All means and contrasts between the groups are represented with their distributions in <a href="#fig-g-violins" class="quarto-xref">Figure 1</a>. Apart from sensory imagery, evidence was found in favour of a difference between the groups on the verbal scale of the OSIVQ, with individuals with aphantasia scoring higher than controls (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math> = -6.39, 95% CrI = [-10.12, -2.53], <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 1.63). No differences between the groups were found on all other variables: the statistical analyses showed evidence against a difference on the spatial scale of the OSIVQ (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -0.7), against differences in Raven matrices scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -1.3), SRI scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -2.2), spatial span (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -1.01), digit span (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -3.28), Similarities test scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -2.68), Reading comprehension scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -1.94) and WCST scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -3.22).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-g-violins" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig"><div aria-describedby="fig-g-violins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="manuscript_files/figure-html/fig-g-violins-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure 1: Standardised scores of the two VVIQ groups on all the questionnaires and tasks. The scores have been rescaled between 0 and 1 to be represented on the same scale. The coloured shapes represent the distribution of the scores in each group. The coloured dots represent the mean of each group, while the bars represent the standard deviations. The stars represent weight of evidence thresholds in favour of an effect of the Group: * = ‘Substantial evidence’, ** = ‘Strong evidence’, *** = ‘Decisive evidence’."><img src="manuscript_files/figure-html/fig-g-violins-1.png" width="1056"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-g-violins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 1: Standardised scores of the two VVIQ groups on all the questionnaires and tasks. The scores have been rescaled between 0 and 1 to be represented on the same scale. The coloured shapes represent the distribution of the scores in each group. The coloured dots represent the mean of each group, while the bars represent the standard deviations. The stars represent weight of evidence thresholds in favour of an effect of the Group: * = ‘<em>Substantial evidence</em>’, ** = ‘<em>Strong evidence</em>’, *** = ‘<em>Decisive evidence</em>’.
</figcaption></figure>
</div>
</div>
</div>
<p>The VVIQ model—i.e., the division of the sample into two VVIQ groups of individuals with aphantasia and controls—therefore had very little explanatory power on task performance. However, large inter-individual variances were observed in various outcomes, as evidenced by the spread of the outcomes’ distributions and several bimodal distributions (e.g., distributions of the OSIVQ-Verbal, SRI, or Reading comprehension scores, see <a href="#fig-g-violins" class="quarto-xref">Figure 1</a>). These unexplained differences suggested the existence of an underlying structure in our sample, thus requiring a better model with more relevant groups to account for them in light of our data. We studied this hypothesis by searching for sub-groups in the sample using data-driven unsupervised clustering.</p>
</section></section><section class="level2" data-number="3.2"><h3 data-number="3.2" id="cluster-analysis">
<span class="header-section-number">3.2</span> Cluster analysis<a class="anchor" aria-label="anchor" href="#cluster-analysis"></a>
</h3>
<section class="level3" data-number="3.2.1"><h4 data-number="3.2.1" id="correlation-structure-and-variable-selection">
<span class="header-section-number">3.2.1</span> Correlation structure and variable selection<a class="anchor" aria-label="anchor" href="#correlation-structure-and-variable-selection"></a>
</h4>
<p>The selection of relevant variables for clustering is essential for good model fit and interpretation of the results <span class="citation" data-cites="fopVariableSelectionMethods2018 zakharovApplicationKmeansClustering2016">(<a href="#ref-fopVariableSelectionMethods2018" role="doc-biblioref">Fop &amp; Murphy, 2018</a>; <a href="#ref-zakharovApplicationKmeansClustering2016" role="doc-biblioref">Zakharov, 2016</a>)</span>. Having an adequate number of dimensions (variables) for a given sample size is also crucial to increase the quality of the clustering <span class="citation" data-cites="psutkaSampleSizeMaximumlikelihood2019">(<a href="#ref-psutkaSampleSizeMaximumlikelihood2019" role="doc-biblioref">Psutka &amp; Psutka, 2019</a>)</span>. The identification and reduction of <em>redundant variables</em> is particularly important, so as not to distort the relative weight of each latent variable in the clustering process. If two variables represent the same concept, that concept would be represented twice in the data and hence get twice the weight as all the other variables. The final solution could be skewed in the direction of that concept, which would considerably compromise the relevance of the model for understanding variable importance <span class="citation" data-cites="kyriazosDealingMulticollinearityFactor2023">(<a href="#ref-kyriazosDealingMulticollinearityFactor2023" role="doc-biblioref">Kyriazos &amp; Poga, 2023</a>)</span>. In the present analysis, this issue particularly affected sensory imagery, which was represented by nine highly correlated variables (VVIQ, OSIVQ-Object, and the seven Psi-Q modalities, Pearson’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.65</mn><mo>,</mo><mn>0.94</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">r \in [0.65, 0.94]</annotation></semantics></math> for every pairwise correlation) that were likely to reflect very similar constructs, as opposed to the remaining nine variables. Several methods exist to deal with such multicollinearity problems. In our low-dimensional setting, we chose to merge variables by averaging them to maintain interpretability while enhancing the stability of the model <span class="citation" data-cites="kyriazosDealingMulticollinearityFactor2023">(<a href="#ref-kyriazosDealingMulticollinearityFactor2023" role="doc-biblioref">Kyriazos &amp; Poga, 2023</a>)</span>. To choose which variables to merge, we analysed the relationships between the variables using partial correlations. Partial correlations measure the degree of association between two variables while controlling for the effect of other potentially confounding covariates <span class="citation" data-cites="abdiPartSemiPartial2007">(<a href="#ref-abdiPartSemiPartial2007" role="doc-biblioref">Abdi, 2007</a>)</span>. This procedure allows to identify the strongest unbiased links between variables and prevents misinterpretation of spurious correlations.</p>
<p>We computed all the partial correlations between the 18 variables (see <a href="#fig-corrs" class="quarto-xref">Figure 2</a>) and chose to merge the significantly correlated variables after a Bonferroni correction (multiplying the <em>p</em>-values by the number of comparisons). This resulted in the creation of four new reduced variables. First, the three subscales related to visual imagery, i.e., the VVIQ, the OSIVQ-Visual and Psi-Q Visual, were associated (VVIQ - Psi-Q Visual: <em>r</em> = 0.67, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.001; OSIVQ-Object - Psi-Q Visual: <em>r</em> = 0.36, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.05). They have been standardised between 0 and 1, weighted by their number of items (16, 15 and 3 respectively) and merged into a single “<em>Visual imagery</em>” variable to obtain as balanced a continuous measure of imagery as possible. Second, the OSIVQ-Spatial and the score of the SRI, i.e., subjective and objective spatial imagery, were associated (<em>r</em> = 0.36, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.05). They have been standardised and merged in a single “<em>Spatial imagery</em>” variable. Third, five Psi-Q sensory imagery subscales (Smell, Taste, Touch, Sensations and Feelings) were associated (Psi-Q Smell - Taste: <em>r</em> = 0.47, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.001; Taste - Touch: <em>r</em> = 0.38, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.05; Touch - Sensations: <em>r</em> = 0.41, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.01; Sensations - Feelings: <em>r</em> = 0.39, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.05). They have been standardised and merged into a single “<em>Sensory imagery</em>” variable. Fourth, the score for the Raven matrices and the Digit span were associated (<em>r</em> = 0.37, <em>p</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.05). They have been standardised and merged in a common “<em>Raven + Digit</em>” variable. Based on the link established between the reverse digit span and measures of intellectual or executive functions <span class="citation" data-cites="groegerMeasuringMemorySpan1999">(<a href="#ref-groegerMeasuringMemorySpan1999" role="doc-biblioref">Groeger et al., 1999</a>)</span>, we interpreted this variable theoretically as a proxy for general cognitive performance.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-corrs" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig"><div aria-describedby="fig-corrs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="manuscript_files/figure-html/fig-corrs-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure 2: Correlation matrix and undirected graph representing the partial correlations between the 18 variables. The stars in the matrix represent p-value thresholds: * : p &lt; 0.05, ** : p &lt; 0.01, *** : p &lt; 0.001, after a Bonferroni correction. The links in the graph represent significant partial correlations. The coloured (non-black) nodes highlight the variables used for clustering, some of which are the result of merging of several correlated variables. Light blue: Visual imagery; dark blue: Sensory imagery; light orange: Spatial imagery; green: Verbal strategies; pink: Raven + Digit; dark orange: Verbal reasoning."><img src="manuscript_files/figure-html/fig-corrs-1.png" width="1344"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-corrs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 2: Correlation matrix and undirected graph representing the partial correlations between the 18 variables. The stars in the matrix represent p-value thresholds: * : p <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.05, ** : p <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.01, *** : p <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.001, after a Bonferroni correction. The links in the graph represent significant partial correlations. The coloured (non-black) nodes highlight the variables used for clustering, some of which are the result of merging of several correlated variables. Light blue: Visual imagery; dark blue: Sensory imagery; light orange: Spatial imagery; green: Verbal strategies; pink: Raven + Digit; dark orange: Verbal reasoning.
</figcaption></figure>
</div>
</div>
</div>
<p>Finally, three variables were not included in the clustering. The Psi-Q Auditory was the only Psi-Q subscale that was not associated with other variables. As it comprises only three items, this variable was not included in the clustering to avoid giving it undue importance. The WCST and Reading comprehension scores were not used either, as these tasks are designed to evaluate higher-level abilities that operate at more integrated levels of cognition. Executive functioning and reading comprehension inextricably involve a mix of working memory, reasoning and attention <span class="citation" data-cites="heatonWisconsinCardSorting1993 kongsWCST64WisconsinCard2000 suggateMentalImagerySkill2022">(<a href="#ref-heatonWisconsinCardSorting1993" role="doc-biblioref">Heaton &amp; Staff, 1993</a>; <a href="#ref-kongsWCST64WisconsinCard2000" role="doc-biblioref">Kongs et al., 2000</a>; <a href="#ref-suggateMentalImagerySkill2022" role="doc-biblioref">Suggate &amp; Lenhard, 2022</a>)</span>, and are likely to integrate many redundant processes with the other assessments. Instead, these variables were used <em>a posteriori</em> as testing variables to assess the generalisability of the cluster model to external variables, i.e., to a related sensory imagery subscale for the Psi-Q Auditory, and to complex cognitive tasks for the other two. This decision was not planned before the study, but it was decided before conducting the cluster analysis based on variable reduction and theoretical considerations.</p>
<p>This entire selection procedure allowed to reduce the variable space to seven dimensions, estimated by <span class="citation" data-cites="psutkaSampleSizeMaximumlikelihood2019">Psutka &amp; Psutka (<a href="#ref-psutkaSampleSizeMaximumlikelihood2019" role="doc-biblioref">2019</a>)</span> to yield a good accuracy of parameter recovery for model-based clustering (see next section) on a sample <em>N</em> = 96. As a result, other variables were not modified to keep as much information as possible. For the sake of clarity, several scores have been renamed to reflect what they assess. The OSIVQ-Verbal score was identified as the propensity to use <em>Verbal strategies</em> for information processing, in line with the definition of this sub-scale <span class="citation" data-cites="blazhenkovaNewObjectspatialverbalCognitive2009">(see <a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">Blazhenkova &amp; Kozhevnikov, 2009</a>)</span>. The Similarities test score was identified as a <em>Verbal Reasoning</em> variable. The clustering process was therefore conducted on the seven following variables: <em>Visual imagery</em>, <em>Sensory imagery</em>, <em>Spatial imagery</em>, <em>Verbal strategies</em>, <em>Raven + Digit span</em>, <em>Verbal reasoning</em> and <em>Spatial span</em>. To model variables using the same scale, data were normalized between 0 and 1 from their respective scales, as recommended by <span class="citation" data-cites="zakharovApplicationKmeansClustering2016">Zakharov (<a href="#ref-zakharovApplicationKmeansClustering2016" role="doc-biblioref">2016</a>)</span>.</p>
</section><section class="level3" data-number="3.2.2"><h4 data-number="3.2.2" id="model-based-clustering-and-number-of-clusters">
<span class="header-section-number">3.2.2</span> Model-based clustering and number of clusters<a class="anchor" aria-label="anchor" href="#model-based-clustering-and-number-of-clusters"></a>
</h4>
<p>A model-based method was chosen for clustering. In this approach, clustering aims at modelling distributions with mixtures of multivariate Gaussian distributions <span class="citation" data-cites="steinleyEvaluatingMixtureModeling2011">(<a href="#ref-steinleyEvaluatingMixtureModeling2011" role="doc-biblioref">Steinley &amp; Brusco, 2011</a>)</span>. Finite Gaussian mixture models (GMM) attempt to determine the underlying population groups that produced the observed data, each cluster being a distribution with its own centre and spread. The resulting model is then used to compute the probability of each observation belonging to a cluster. Although discrete (k-means) or hierarchical clustering methods are frequently used in psychology <span class="citation" data-cites="zakharovApplicationKmeansClustering2016">(<a href="#ref-zakharovApplicationKmeansClustering2016" role="doc-biblioref">Zakharov, 2016</a>)</span>, probabilistic mixture modelling approaches have proven to be more powerful and parsimonious with partially overlapping, non-spherical, multivariate normal distributions, and small sample sizes, all of which are common in psychology experiments <span class="citation" data-cites="dalmaijerStatisticalPowerCluster2022">(<a href="#ref-dalmaijerStatisticalPowerCluster2022" role="doc-biblioref">Dalmaijer et al., 2022</a>)</span>.</p>
<p>Given that very little information is available on the clusters, the estimation of the GMM proceeds in steps, alternating between (1) estimating the posterior probability of each observation belonging to each cluster with a fixed set of parameters and (2) updating the estimates of the parameters by fixing the probability of cluster membership for each observation <span class="citation" data-cites="steinleyEvaluatingMixtureModeling2011">(<a href="#ref-steinleyEvaluatingMixtureModeling2011" role="doc-biblioref">Steinley &amp; Brusco, 2011</a>)</span>. This iterative procedure continues until the model converges on stable clusters. The standard method for this estimation is the expectation-maximisation algorithm <span class="citation" data-cites="dempsterMaximumLikelihoodIncomplete1977">(EM, <a href="#ref-dempsterMaximumLikelihoodIncomplete1977" role="doc-biblioref">Dempster et al., 1977</a>)</span><a class="footnote-ref" role="doc-noteref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Mathematical and technical details can be found in the documentation for the &lt;em&gt;mclust&lt;/em&gt; R package &lt;span class="citation" data-cites="mclust2023"&gt;(&lt;a href="#ref-mclust2023" role="doc-biblioref"&gt;Scrucca et al., 2023&lt;/a&gt;)&lt;/span&gt;, among others.&lt;/p&gt;'><sup>1</sup></a>. In the present study, the <em>mclust</em> R package <span class="citation" data-cites="mclust2023">(<a href="#ref-mclust2023" role="doc-biblioref">Scrucca et al., 2023</a>)</span> was used to conduct GMM clustering. The estimation procedure for the mixture of clusters in the GMM requires knowledge of the number of clusters and their distributional form. The determination of these parameters was done using the Bayesian Information Criterion (BIC) implemented in the <em>mclust</em> package.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-bic" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig"><div aria-describedby="fig-bic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="manuscript_files/figure-html/fig-bic-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure 3: Comparison of the goodness of fit of different mixture models used for clustering as a function of model type and number of components. A high BIC indicates a good model fit. The three-letter acronyms describe the components of the mixture models. The first letter describes the volume of the components, the second their shape and the last their orientation. E = equal, V = variable. Acronyms ending with ‘II’ indicate mixtures of spherical components, those ending with ‘I’ indicate mixtures of diagonal components and those without ‘I’ indicate mixtures of ellipsoidal components."><img src="manuscript_files/figure-html/fig-bic-1.png" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 3: Comparison of the goodness of fit of different mixture models used for clustering as a function of model type and number of components. A high BIC indicates a good model fit. The three-letter acronyms describe the components of the mixture models. The first letter describes the volume of the components, the second their shape and the last their orientation. E = equal, V = variable. Acronyms ending with ‘II’ indicate mixtures of spherical components, those ending with ‘I’ indicate mixtures of diagonal components and those without ‘I’ indicate mixtures of ellipsoidal components.
</figcaption></figure>
</div>
</div>
</div>
<p>There are no generally accepted rules regarding minimum sample sizes in clustering procedures. For model-based clustering procedures <span class="citation" data-cites="dalmaijerStatisticalPowerCluster2022">Dalmaijer et al. (<a href="#ref-dalmaijerStatisticalPowerCluster2022" role="doc-biblioref">2022</a>)</span> recommended <em>N</em> = 20 to <em>N</em> = 30 per expected cluster with medium to large effect sizes (i.e., cluster separation), which would translate into two to four clusters in our sample. Given the present dataset, the comparison of various GMM types with different number of clusters using the BIC showed that the best solution was a model with three ellipsoidal clusters of varying shapes, equal volume and orientations (EVE model, see <a href="#fig-bic" class="quarto-xref">Figure 3</a>).</p>
</section><section class="level3" data-number="3.2.3"><h4 data-number="3.2.3" id="clustering-results">
<span class="header-section-number">3.2.3</span> Clustering results<a class="anchor" aria-label="anchor" href="#clustering-results"></a>
</h4>
<p>The profiles of the three clusters obtained are presented in <a href="#fig-cluster-radar-plots" class="quarto-xref">Figure 4</a> (left panel). <strong>Cluster A</strong> was made up exclusively of individuals with aphantasia (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>N</mi><mi>A</mi></msub><annotation encoding="application/x-tex">N_A</annotation></semantics></math> = 32), <strong>Cluster B</strong> was mixed (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>N</mi><mi>B</mi></msub><annotation encoding="application/x-tex">N_B</annotation></semantics></math> = 30), comprising 17 controls and 13 individuals with aphantasia, while <strong>Cluster C</strong> was composed solely of control participants (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>N</mi><mi>C</mi></msub><annotation encoding="application/x-tex">N_C</annotation></semantics></math> = 34). Bayes factors for independence found no association between clusters and levels of education (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -7.44), clusters and fields of study (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -6.06), or clusters and occupation (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -3.88).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cluster-radar-plots" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig"><div aria-describedby="fig-cluster-radar-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="manuscript_files/figure-html/fig-cluster-radar-plots-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure 4: Profiles of the clusters obtained through model-based clustering. Left: Polar plot representing the standardised means (points) and standard errors (error bars) of the three clusters on the seven clustering variables and the three external variables (Auditory imagery, WCST and Reading comprehension). Cluster A is exclusively composed of individuals with aphantasia, Cluster C is exclusively composed of control participants, and Cluster B is mixed. Right: Same polar plot with Cluster B divided into two sub-clusters, ‘B-Aphant.’ with the 13 individuals with aphantasia in B, and ‘B-Control’ with the 17 control participants in B."><img src="manuscript_files/figure-html/fig-cluster-radar-plots-1.png" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cluster-radar-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 4: Profiles of the clusters obtained through model-based clustering. <strong>Left:</strong> Polar plot representing the standardised means (points) and standard errors (error bars) of the three clusters on the seven clustering variables and the three external variables (Auditory imagery, WCST and Reading comprehension). Cluster A is exclusively composed of individuals with aphantasia, Cluster C is exclusively composed of control participants, and Cluster B is mixed. <strong>Right:</strong> Same polar plot with Cluster B divided into two sub-clusters, ‘B-Aphant.’ with the 13 individuals with aphantasia in B, and ‘B-Control’ with the 17 control participants in B.
</figcaption></figure>
</div>
</div>
</div>
<p>Cluster scores were modelled with the clusters as predictors using the same linear Bayesian modelling as described above for modelling variables with the groups. All detailed pairwise differences between the clusters and their 95% Credible Intervals are reported in <a href="#tbl-c-contrasts" class="quarto-xref">Table 2</a> along with weights of evidence in favour of differences between clusters. The three clusters had very distinctive features on the visual, sensory, spatial and verbal dimensions of their cognitive profiles. As seen, for Visual and Sensory imagery, Controls (cluster C) had much higher scores than the mixed cluster (cluster B) which itself had higher scores than the Aphantasic cluster (cluster A; all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \geq</annotation></semantics></math> 3.99). Cluster B had higher scores than the other two clusters in Spatial imagery (both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \geq</annotation></semantics></math> 5.52), which did not differ from each other (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -1.74). In turn, cluster A had higher scores than the other two in Verbal strategies (both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \geq</annotation></semantics></math> 4.31), for which B and C did not differ (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -3.19). Else, cluster B outperformed cluster C in verbal reasoning and spatial span (both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \geq</annotation></semantics></math> 3.33), but no differences were found between cluster A and the other two on these scores (all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \leq</annotation></semantics></math> 0.16). Finally, for the Raven + Digit span variable, the differences between the clusters were negligible (all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">log(BF_{10}) \leq</annotation></semantics></math> -2.2).</p>
<p>We also modelled the variables excluded from the clustering procedure, i.e., Auditory imagery, WCST and reading comprehension scores, to assess how the cluster patterns transferred to external variables. They are seen at the bottom of <a href="#tbl-c-contrasts" class="quarto-xref">Table 2</a>. The imagery patterns were maintained for Auditory imagery, where cluster C scored higher than B (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 2.33), which in turn scored higher than A (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 11.37). However, although the mean cluster scores on the WCST and reading comprehension followed the same trend as the other cognitive tasks (i.e. B <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> A <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> C — WCST: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>B</mi></msub><annotation encoding="application/x-tex">M_{B}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">SD_{B}</annotation></semantics></math>) = 0.66 (0.14); <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>A</mi></msub><annotation encoding="application/x-tex">M_{A}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">SD_{A}</annotation></semantics></math>) = 0.65 (0.13); <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>C</mi></msub><annotation encoding="application/x-tex">M_{C}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mi>C</mi></msub></mrow><annotation encoding="application/x-tex">SD_{C}</annotation></semantics></math>) = 0.6 (0.16) — Reading comprehension: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>B</mi></msub><annotation encoding="application/x-tex">M_{B}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">SD_{B}</annotation></semantics></math>) = 0.63 (0.2); <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>A</mi></msub><annotation encoding="application/x-tex">M_{A}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">SD_{A}</annotation></semantics></math>) = 0.6 (0.16); <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mi>C</mi></msub><annotation encoding="application/x-tex">M_{C}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>D</mi><mi>C</mi></msub></mrow><annotation encoding="application/x-tex">SD_{C}</annotation></semantics></math>) = 0.52 (0.23)), there was evidence for an absence of differences between clusters (highest <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -1.26).</p>
<p>Dividing the controls and individuals with aphantasia within cluster B into two “sub-clusters” provided further insights on the profiles in the sample (see <a href="#fig-cluster-radar-plots" class="quarto-xref">Figure 4</a>, right panel; details of all 60 pairwise comparisons can be found in the extended analysis report on the OSF at <a href="https://osf.io/7vsx6/" class="external-link uri">https://osf.io/7vsx6/</a>). This allowed to assess imagery differences that might have been biased by averaging imagery scores in this mixed cluster. First, comparisons between controls from B (noted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>C</mi></msub><annotation encoding="application/x-tex">B_{C}</annotation></semantics></math>) and from C showed that C had higher visual imagery than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>C</mi></msub><annotation encoding="application/x-tex">B_{C}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 6.96), but comparable sensory imagery (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -0.78) and auditory imagery scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -2.36). As observed above, B-Controls had higher spatial imagery (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 9.56), spatial span (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 1.53) and verbal reasoning scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 0.76), but no differences in verbal strategies (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -3.31) or Raven + Digit span (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -3.35). These comparisons show that the main imagery differences between the B and C controls lie in visual and spatial imagery, indicating that the differences in performance observed in the tasks could be related to the balance between the two, a high visual imagery being associated with lower performance.</p>
<p>Last, comparisons between individuals with aphantasia from A and from B (noted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>A</mi></msub><annotation encoding="application/x-tex">B_{A}</annotation></semantics></math>) confirmed their differences in verbal strategies in favour of A (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 3.79), and in spatial imagery in favour of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>A</mi></msub><annotation encoding="application/x-tex">B_{A}</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 1.16), along with an absence of differences in Raven + Digit span (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -0.88), spatial span (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -2.02), and verbal reasoning (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -1.18). However, the contrasts also revealed that, although the two had comparable visual imagery (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = -4.02), <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>A</mi></msub><annotation encoding="application/x-tex">B_{A}</annotation></semantics></math> had higher sensory imagery than A (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 2.46). This difference also existed in auditory imagery (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> = 1.35), which was not used as a variable for clustering. The differences in verbal strategies, spatial imagery, sensory and auditory imagery between A and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>A</mi></msub><annotation encoding="application/x-tex">B_{A}</annotation></semantics></math> aphants point to the existence of aphantasia subgroups with major mental imagery and cognitive style differences extending beyond visual imagery.</p>
</section></section></section><section class="section level2" data-number="4"><h2 data-number="4" id="discussion">
<span class="header-section-number">4</span> Discussion<a class="anchor" aria-label="anchor" href="#discussion"></a>
</h2>
<p>The present study investigated the cognitive profiles of individuals with and without aphantasia and how these profiles influence their performance on various cognitive tasks. To this end, we examined key aspects of subjective experiences, including visual and sensory imagery, spatial imagery, and verbal strategies, using questionnaires. Additionally, reasoning, working memory, executive functioning, and reading comprehension were assessed through a battery of behavioural tasks.</p>
<p>The initial analyses, comparing individuals with aphantasia to controls based on conventional measures, revealed few significant differences in performance. While individuals with aphantasia demonstrated reduced sensory imagery and a stronger reliance on verbal strategies, differences in spatial imagery and other cognitive tasks, such as reasoning or memory, were minimal. These findings aligned with previous research emphasizing the limited impact of aphantasia in many behavioural tasks <span class="citation" data-cites="bainbridgeQuantifyingAphantasiaDrawing2021 keoghVisualWorkingMemory2021 knightMemoryImageryNo2022 pounderOnlyMinimalDifferences2022">(e.g., <a href="#ref-bainbridgeQuantifyingAphantasiaDrawing2021" role="doc-biblioref">Bainbridge et al., 2021</a>; <a href="#ref-keoghVisualWorkingMemory2021" role="doc-biblioref">Keogh et al., 2021</a>; <a href="#ref-knightMemoryImageryNo2022" role="doc-biblioref">Knight et al., 2022</a>; <a href="#ref-pounderOnlyMinimalDifferences2022" role="doc-biblioref">Pounder et al., 2022</a>)</span>. However, a clustering analysis using a Gaussian Mixture Model (GMM) identified a cluster structure within the dataset. By leveraging variables related to visual, sensory, and spatial imagery, verbal strategies, and cognitive abilities (i.e., Raven matrices, digit span, spatial span, and verbal reasoning), the algorithm suggested the presence of three distinct cognitive profiles, each with unique characteristics in visual, sensory, and spatial imagery, as well as verbal strategies. One cluster, exclusively composed of controls, demonstrated high visual and sensory imagery, strongly relying on visual-object representations. Another mixed cluster, including both individuals with aphantasia and controls, exhibited strong spatial imagery and performed best on spatial span and verbal reasoning tasks. The final cluster consisted entirely of individuals with aphantasia, who displayed multisensory aphantasia (reduced sensory, auditory, and visual imagery) and a reliance on verbal strategies. This clustering revealed complex, multidimensional patterns of cognitive abilities and imagery preferences that extended beyond the binary categorization of aphantasia versus control, offering a rich and nuanced picture of the relationships between mental imagery and cognition.</p>
<p>If we focus only on the Control participants, the cluster analyses revealed important subgroup distinctions. Controls could be divided into two subgroups: one with highly vivid visual imagery but reduced spatial imagery (cluster C) and another with the opposite pattern, characterized by stronger spatial imagery (cluster <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>C</mi></msub><annotation encoding="application/x-tex">B_{C}</annotation></semantics></math>). These distinctions held important implications for task performance. Individuals with higher spatial imagery outperformed their counterparts with stronger visual imagery in tasks requiring reasoning and working memory, such as spatial span and verbal reasoning tasks. Interestingly, both groups performed similarly on tasks involving non-verbal reasoning (Raven matrices) and reverse digit span, suggesting that the observed differences were not due to overall disparities in cognitive ability but were specific to differences in imagery style. These results emphasize the importance of spatial imagery in abstract reasoning and suggest that vivid visual imagery may sometimes impede these processes. This is consistent with the “visual imagery impedance hypothesis” <span class="citation" data-cites="knauffVisualImageryCan2002">(<a href="#ref-knauffVisualImageryCan2002" role="doc-biblioref">Knauff &amp; Johnson-Laird, 2002</a>)</span>, which posits that overly detailed visual representations can disrupt reasoning by introducing irrelevant information. These findings not only reinforce the relevance of spatial imagery in cognitive profiles but also raise questions about its relationship with visual imagery, highlighting potential trade-offs between visual and spatial imagery capacities, a dynamic previously suggested by <span class="citation" data-cites="kozhevnikovTradeoffObjectSpatial2010">Kozhevnikov et al. (<a href="#ref-kozhevnikovTradeoffObjectSpatial2010" role="doc-biblioref">2010</a>)</span>.</p>
<p>The clustering approach also revealed important heterogeneity within the group of individuals with aphantasia. While a shared absence of visual imagery defined all participants with aphantasia, two distinct subgroups emerged: one that relied heavily on spatial strategies (“spatialisers”) and another that depended predominantly on verbal processing (“verbalisers”). Spatialisers demonstrated preserved sensory and auditory imagery and demonstrated strong performance in tasks that emphasized spatial representation, such as mental rotation and spatial manipulation tasks. Verbalisers, by contrast, exhibited what could be described as “multisensory aphantasia” reporting not only the absence of visual imagery but also reduced or absent sensory and auditory imagery. This group relied heavily on verbal strategies, with a preference for linguistic forms of information processing. The distinction between “multisensory aphantasia” and “visual-only aphantasia” aligns with a prior clustering study conducted by <span class="citation" data-cites="dawesMultisensorySubtypesAphantasia2023">Dawes et al. (<a href="#ref-dawesMultisensorySubtypesAphantasia2023" role="doc-biblioref">2023</a>)</span>, which highlighted the heterogeneity of aphantasia across larger samples. However, our findings uncover another relevant dimension by linking these imagery profiles to different cognitive styles: verbal versus spatial. The strong performance of spatialisers on spatial tasks supports prior work suggesting that many classic “mental imagery tasks”, such as the Mental Rotation Task <span class="citation" data-cites="shepardMentalRotationThreeDimensional1971">(<a href="#ref-shepardMentalRotationThreeDimensional1971" role="doc-biblioref">Shepard &amp; Metzler, 1971</a>)</span>, or the Paper Folding Test <span class="citation" data-cites="ekstromKitFactorreferencedCognitive1976">(<a href="#ref-ekstromKitFactorreferencedCognitive1976" role="doc-biblioref">Ekstrom, 1976</a>)</span> rely more heavily on spatial than visual-object imagery <span class="citation" data-cites="kozhevnikovSpatialObjectVisualizers2005 blazhenkovaNewObjectspatialverbalCognitive2009 borstIndividualDifferencesSpatial2010 kozhevnikovTradeoffObjectSpatial2010 haciomerogluObjectSpatialVisualizationVerbal2016 bledValidationFrenchVersion2021">(<a href="#ref-blazhenkovaNewObjectspatialverbalCognitive2009" role="doc-biblioref">Blazhenkova &amp; Kozhevnikov, 2009</a>; <a href="#ref-bledValidationFrenchVersion2021" role="doc-biblioref">Bled &amp; Bouvet, 2021</a>; <a href="#ref-borstIndividualDifferencesSpatial2010" role="doc-biblioref">Borst &amp; Kosslyn, 2010</a>; <a href="#ref-haciomerogluObjectSpatialVisualizationVerbal2016" role="doc-biblioref">Haciomeroglu, 2016</a>; e.g., <a href="#ref-kozhevnikovSpatialObjectVisualizers2005" role="doc-biblioref">Kozhevnikov et al., 2005</a>; <a href="#ref-kozhevnikovTradeoffObjectSpatial2010" role="doc-biblioref">Kozhevnikov et al., 2010</a>)</span>. Furthermore, these findings on cognitive styles provide insights into why individuals with aphantasia perform well on spatial reasoning tasks, echoing earlier work on the amodal nature of spatial representations <span class="citation" data-cites="johnson-lairdMentalModelsHuman2010">(<a href="#ref-johnson-lairdMentalModelsHuman2010" role="doc-biblioref">Johnson-Laird, 2010</a>)</span> and spatial imagery in aphantasia <span class="citation" data-cites="palermoCongenitalLackExtraordinary2022">(<a href="#ref-palermoCongenitalLackExtraordinary2022" role="doc-biblioref">Palermo et al., 2022</a>)</span>. Nuances in spatial imagery and cognitive styles among individuals with aphantasia may also be related to the presence or absence of unconscious visual representations, a phenomenon currently debated in the literature <span class="citation" data-cites="krempelAphantasiaInvoluntaryImagery2024 murakiInsightsEmbodiedCognition2023 purkartAreThereUnconscious2024">(see <a href="#ref-krempelAphantasiaInvoluntaryImagery2024" role="doc-biblioref">Krempel &amp; Monzel, 2024</a>; <a href="#ref-murakiInsightsEmbodiedCognition2023" role="doc-biblioref">Muraki et al., 2023</a>; <a href="#ref-purkartAreThereUnconscious2024" role="doc-biblioref">Purkart et al., 2024</a>)</span>. In this respect, unsupervised clustering could unveil previously unrecognised heterogeneities within visual imagery groups, offering novel insights into the cognitive architectures underlying mental representations.</p>
<p>The spatial and verbal profiles observed in aphantasia profiles could also have broader implications beyond cognitive processes and task performance. <span class="citation" data-cites="zemanPhantasiaPsychologicalSignificance2020">Zeman et al. (<a href="#ref-zemanPhantasiaPsychologicalSignificance2020" role="doc-biblioref">2020</a>)</span> have noted a disproportionate representation of individuals with aphantasia in STEM fields, which may reflect the influence of these spatial profiles among individuals with visual aphantasia. Indeed, numerous studies on the associations between the OSIVQ and categories of occupation, study and activity have found correlations between the three visual-object, spatial and verbal dimensions and areas of specialisation in visual arts, science and humanities, respectively <span class="citation" data-cites="blazhenkovaTwoEyesBlind2019">(for an extended review, see <a href="#ref-blazhenkovaTwoEyesBlind2019" role="doc-biblioref">Blazhenkova &amp; Pechenkova, 2019</a>)</span>. These patterns suggest that cognitive profiles encompassing spatial and verbal dimensions could be relevant to understand the consequences of aphantasia in ecological contexts, as supported by recent findings linking OSIVQ-derived profiles to real-world problem-solving abilities <span class="citation" data-cites="hofflerMoreEvidenceThree2017 chkhaidzeIndividualDifferencesPreferred2023">(<a href="#ref-chkhaidzeIndividualDifferencesPreferred2023" role="doc-biblioref">Chkhaidze et al., 2023</a>; <a href="#ref-hofflerMoreEvidenceThree2017" role="doc-biblioref">Höffler et al., 2017</a>)</span>. Aphantasia may represent one extreme among many in a larger, multidimensional cognitive spectrum, where individual differences in imagery and cognitive style influence both cognitive abilities, subjective experience and daily life.</p>
<p>Since the first systematic investigation of aphantasia <span class="citation" data-cites="zemanLivesImageryCongenital2015">(<a href="#ref-zemanLivesImageryCongenital2015" role="doc-biblioref">Zeman et al., 2015</a>)</span>, individuals with this condition have reported “compensatory strengths in verbal, mathematical and logical domains”, but our understanding of these cognitive strengths and weaknesses associated with aphantasia remains incomplete. While the profiles identified in this study offer a potential framework to explore these compensatory mechanisms further, several limitations must be acknowledged. The use of an unsupervised clustering algorithm allowed for the discovery of hidden structures in the data, but the findings remain exploratory and require replication in larger, more diverse samples. Moreover, the tasks used to assess mental imagery and reasoning were selected to provide broad coverage of cognitive abilities but may not have captured the full complexity of these constructs. Future research should aim to refine the measurement of visual, spatial, and verbal imagery through the use of tasks specifically designed to disentangle these dimensions. Furthermore, it remains to be seen whether the profiles identified here can be validated with other behavioural and neural phenomena, such as differences in brain activity during visual or spatial imagery tasks.</p>
<p>Overall, the findings of this study suggest that variations in visual, spatial, and verbal cognitive styles offer a rich framework for understanding the “phantasia continuum”. This framework not only highlights the heterogeneity within the aphantasia population but also situates aphantasia within a broader spectrum of cognitive abilities. Ultimately, by moving beyond the binary classification of aphantasia and exploring the multidimensional nature of cognitive profiles, this study aims to contribute to a more nuanced and cohesive understanding of the diversity of mental imagery and its cognitive implications. In turn, the spectacular variability in inner experiences represents an invaluable source of information into mental representations, which could answer long-standing questions about their nature, from their modal and amodal properties to their interaction with fundamental processes such as reasoning, problem-solving or working memory.</p>
</section><section class="section level2"><h2 class="unnumbered" id="research-transparency-statement">Research transparency statement<a class="anchor" aria-label="anchor" href="#research-transparency-statement"></a>
</h2>
<p>All the following elements required to reproduce the study and analyses are publicly available on the Open Science Framework (<a href="https://osf.io/7vsx6/" class="external-link uri">https://osf.io/7vsx6/</a>): all online study materials; all anonymised primary data and pre-processed data; all analysis code and notebooks with extensive commentary and supplementary information on the exploratory analysis process and results. No artificial intelligence assisted technologies were used in this research or the creation of this article.</p>
</section><section class="section level2"><h2 class="unnumbered" id="author-contributions">Author contributions<a class="anchor" aria-label="anchor" href="#author-contributions"></a>
</h2>
<p>Conceptualisation: MD, ST, EC, GP. Data curation: MD. Formal analysis: MD. Funding acquisition: GP. Investigation: MD, ST. Methodology: MD, ST, DC, EC, GP. Project administration: GP, EC. Resources: MD, ST, EC, DC. Software: MD. Supervision: GP, EC. Visualisation: MD. Writing - Original Draft Preparation: MD. Writing - Review &amp; Editing: GP, DC, EC.</p>
</section><section class="section level2"><h2 class="unnumbered" id="declaration-of-interests">Declaration of interests<a class="anchor" aria-label="anchor" href="#declaration-of-interests"></a>
</h2>
<p>None.</p>
</section><section class="section level2"><h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-abdiPartSemiPartial2007" class="csl-entry" role="listitem">
Abdi, H. (2007). Part (semi partial) and partial regression coefficients. <em>Encyclopedia of Measurement and Statistics</em>, <em>3</em>, 1–9.
</div>
<div id="ref-andradeAssessingVividnessMental2014" class="csl-entry" role="listitem">
Andrade, J., May, J., Deeprose, C., Baugh, S.-J., &amp; Ganis, G. (2014). Assessing vividness of mental imagery: The plymouth sensory imagery questionnaire. <em>British Journal of Psychology</em>, <em>105</em>(4), 547–563. <a href="https://doi.org/10.1111/bjop.12050" class="external-link">https://doi.org/10.1111/bjop.12050</a>
</div>
<div id="ref-bainbridgeQuantifyingAphantasiaDrawing2021" class="csl-entry" role="listitem">
Bainbridge, W. A., Pounder, Z., Eardley, A. F., &amp; Baker, C. I. (2021). Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory. <em>Cortex</em>, <em>135</em>, 159–172. <a href="https://doi.org/10.1016/j.cortex.2020.11.014" class="external-link">https://doi.org/10.1016/j.cortex.2020.11.014</a>
</div>
<div id="ref-bartolomeoRelationshipVisualPerception2002" class="csl-entry" role="listitem">
Bartolomeo, P. (2002). The relationship between visual perception and visual mental imagery: A reappraisal of the neuropsychological evidence. <em>Cortex</em>, <em>38</em>(3), 357–378. <a href="https://doi.org/10.1016/S0010-9452(08)70665-8" class="external-link">https://doi.org/10.1016/S0010-9452(08)70665-8</a>
</div>
<div id="ref-bilkerDevelopmentAbbreviatedNineItem2012" class="csl-entry" role="listitem">
Bilker, W. B., Hansen, J. A., Brensinger, C. M., Richard, J., Gur, R. E., &amp; Gur, R. C. (2012). Development of abbreviated nine-item forms of the raven’s standard progressive matrices test. <em>Assessment</em>, <em>19</em>(3), 354–369. <a href="https://doi.org/10.1177/1073191112446655" class="external-link">https://doi.org/10.1177/1073191112446655</a>
</div>
<div id="ref-blackburnRevisedAdministrationScoring1957" class="csl-entry" role="listitem">
Blackburn, H. L., &amp; Benton, A. L. (1957). Revised administration and scoring of the digit span test. <em>Journal of Consulting Psychology</em>, <em>21</em>(2), 139.
</div>
<div id="ref-blazhenkovaNewObjectspatialverbalCognitive2009" class="csl-entry" role="listitem">
Blazhenkova, O., &amp; Kozhevnikov, M. (2009). The new object-spatial-verbal cognitive style model: Theory and measurement. <em>Applied Cognitive Psychology</em>, <em>23</em>(5), 638–663. <a href="https://doi.org/10.1002/acp.1473" class="external-link">https://doi.org/10.1002/acp.1473</a>
</div>
<div id="ref-blazhenkovaVisualobjectAbilityNew2010" class="csl-entry" role="listitem">
Blazhenkova, O., &amp; Kozhevnikov, M. (2010). Visual-object ability: A new dimension of non-verbal intelligence. <em>Cognition</em>, <em>117</em>(3), 276–301. <a href="https://doi.org/10.1016/j.cognition.2010.08.021" class="external-link">https://doi.org/10.1016/j.cognition.2010.08.021</a>
</div>
<div id="ref-blazhenkovaObjectSpatialImagery2006" class="csl-entry" role="listitem">
Blazhenkova, O., Kozhevnikov, M., &amp; Motes, M. A. (2006). Object and spatial imagery: Distinctions between members of different professions. <em>Cognitive Processing</em>, <em>7</em>(1), 20–21. <a href="https://doi.org/10.1007/s10339-006-0047-9" class="external-link">https://doi.org/10.1007/s10339-006-0047-9</a>
</div>
<div id="ref-blazhenkovaTwoEyesBlind2019" class="csl-entry" role="listitem">
Blazhenkova, O., &amp; Pechenkova, E. (2019). The two eyes of the blind mind: Object vs. Spatial aphantasia? <em>Russian Journal of Cognitive Science</em>, <em>6</em>(44), 51–65. <a href="http://dx.doi.org/10.47010/19.4.5" class="external-link">http://dx.doi.org/10.47010/19.4.5</a>
</div>
<div id="ref-bledValidationFrenchVersion2021" class="csl-entry" role="listitem">
Bled, C., &amp; Bouvet, L. (2021). Validation of the french version of the object spatial imagery and verbal questionnaire. <em>European Review of Applied Psychology</em>, <em>71</em>(4), 100687. <a href="https://doi.org/10.1016/j.erap.2021.100687" class="external-link">https://doi.org/10.1016/j.erap.2021.100687</a>
</div>
<div id="ref-borstIndividualDifferencesSpatial2010" class="csl-entry" role="listitem">
Borst, G., &amp; Kosslyn, S. M. (2010). Individual differences in spatial mental imagery. <em>Quarterly Journal of Experimental Psychology</em>, <em>63</em>(10), 2031–2050. <a href="https://doi.org/10.1080/17470211003802459" class="external-link">https://doi.org/10.1080/17470211003802459</a>
</div>
<div id="ref-brethesTextReadingFluency2022" class="csl-entry" role="listitem">
Brèthes, H., Cavalli, E., Denis-Noël, A., Melmi, J.-B., El Ahmadi, A., Bianco, M., &amp; Colé, P. (2022). Text reading fluency and text reading comprehension do not rely on the same abilities in university students with and without dyslexia. <em>Frontiers in Psychology</em>, <em>13</em>. <a href="https://doi.org/10.3389/fpsyg.2022.866543" class="external-link">https://doi.org/10.3389/fpsyg.2022.866543</a>
</div>
<div id="ref-cabbaiInvestigatingRelationshipsTrait2023" class="csl-entry" role="listitem">
Cabbai, G., Dance, C., Dienes, Z., Simner, J., Forster, S., &amp; Lush, P. (2023). <em>Investigating relationships between trait visual imagery and phenomenological control: The role of context effects</em>.
</div>
<div id="ref-chkhaidzeIndividualDifferencesPreferred2023" class="csl-entry" role="listitem">
Chkhaidze, A., Coulson, S., &amp; Kiyonaga, A. (2023). <em>Individual differences in preferred thought formats predict features of narrative recall</em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/xmf7w" class="external-link">https://doi.org/10.31234/osf.io/xmf7w</a>
</div>
<div id="ref-superb2021" class="csl-entry" role="listitem">
Cousineau, D., Goulet, M.-A., &amp; Harding, B. (2021). Summary plots with adjusted error bars: The superb framework with an implementation in <span>R</span>. <em>Advances in Methods and Practices in Psychological Science</em>, <em>4</em>(3), 1–46. <a href="https://doi.org/10.1177/25152459211035109" class="external-link">https://doi.org/10.1177/25152459211035109</a>
</div>
<div id="ref-dalmaijerStatisticalPowerCluster2022" class="csl-entry" role="listitem">
Dalmaijer, E. S., Nord, C. L., &amp; Astle, D. E. (2022). Statistical power for cluster analysis. <em>BMC Bioinformatics</em>, <em>23</em>(1), 205. <a href="https://doi.org/10.1186/s12859-022-04675-1" class="external-link">https://doi.org/10.1186/s12859-022-04675-1</a>
</div>
<div id="ref-danceWhatLinkMental2021" class="csl-entry" role="listitem">
Dance, C. J., Ward, J., &amp; Simner, J. (2021). What is the link between mental imagery and sensory sensitivity? Insights from aphantasia. <em>Perception</em>, <em>50</em>(9), 757–782. <a href="https://doi.org/10.1177/03010066211042186" class="external-link">https://doi.org/10.1177/03010066211042186</a>
</div>
<div id="ref-dawesCognitiveProfileMultisensory2020" class="csl-entry" role="listitem">
Dawes, A., Keogh, R., Andrillon, T., &amp; Pearson, J. (2020). A cognitive profile of multi-sensory imagery, memory and dreaming in aphantasia. <em>Scientific Reports</em>, <em>10</em>(11), 10022. <a href="https://doi.org/10.1038/s41598-020-65705-7" class="external-link">https://doi.org/10.1038/s41598-020-65705-7</a>
</div>
<div id="ref-dawesMultisensorySubtypesAphantasia2023" class="csl-entry" role="listitem">
Dawes, A., Keogh, R., &amp; Pearson, J. (2023). Multisensory subtypes of aphantasia: Mental imagery as supramodal perception in reverse. <em>Neuroscience Research</em>. <a href="https://doi.org/10.1016/j.neures.2023.11.009" class="external-link">https://doi.org/10.1016/j.neures.2023.11.009</a>
</div>
<div id="ref-dawesMemoriesBlindMind2022" class="csl-entry" role="listitem">
Dawes, A., Keogh, R., Robuck, S., &amp; Pearson, J. (2022). Memories with a blind mind: Remembering the past and imagining the future with aphantasia. <em>Cognition</em>, <em>227</em>, 105192. <a href="https://doi.org/10.1016/j.cognition.2022.105192" class="external-link">https://doi.org/10.1016/j.cognition.2022.105192</a>
</div>
<div id="ref-dempsterMaximumLikelihoodIncomplete1977" class="csl-entry" role="listitem">
Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, <em>39</em>(1), 1–22.
</div>
<div id="ref-dupontExplicitImplicitMotor2024" class="csl-entry" role="listitem">
Dupont, W., Papaxanthis, C., Madden-Lombardi, C., &amp; Lebon, F. (2024). Explicit and implicit motor simulations are impaired in individuals with aphantasia. <em>Brain Communications</em>, <em>6</em>(2), fcae072.
</div>
<div id="ref-ekstromKitFactorreferencedCognitive1976" class="csl-entry" role="listitem">
Ekstrom, R. B. (1976). <em>Kit of factor-referenced cognitive tests</em>. Educational Testing Service.
</div>
<div id="ref-farahCaseStudyMental1988" class="csl-entry" role="listitem">
Farah, M. J., Levine, D. N., &amp; Calvanio, R. (1988). A case study of mental imagery deficit. <em>Brain and Cognition</em>, <em>8</em>(2), 147–164. <a href="https://doi.org/10.1016/0278-2626(88)90046-2" class="external-link">https://doi.org/10.1016/0278-2626(88)90046-2</a>
</div>
<div id="ref-fopVariableSelectionMethods2018" class="csl-entry" role="listitem">
Fop, M., &amp; Murphy, T. B. (2018). Variable selection methods for model-based clustering. <em>Statistics Surveys</em>, <em>12</em>(none), 18–65. <a href="https://doi.org/10.1214/18-SS119" class="external-link">https://doi.org/10.1214/18-SS119</a>
</div>
<div id="ref-R-rstanarm" class="csl-entry" role="listitem">
Gabry, J., &amp; Goodrich, B. (2024). <em>Rstanarm: Bayesian applied regression modeling via stan</em>. <a href="https://mc-stan.org/rstanarm/" class="external-link">https://mc-stan.org/rstanarm/</a>
</div>
<div id="ref-gibeauCorsiBlocksTask2021" class="csl-entry" role="listitem">
Gibeau, R.-M. (2021). The corsi blocks task: Variations and coding with jsPsych. <em>The Quantitative Methods for Psychology</em>, <em>17</em>(3), 299–311. <a href="https://doi.org/10.20982/tqmp.17.3.p299" class="external-link">https://doi.org/10.20982/tqmp.17.3.p299</a>
</div>
<div id="ref-goodWeightEvidenceBrief1985" class="csl-entry" role="listitem">
Good, I. J. (1985). Weight of evidence: A brief survey. <em>Bayesian Statistics</em>, <em>2</em>, 249–270.
</div>
<div id="ref-groegerMeasuringMemorySpan1999" class="csl-entry" role="listitem">
Groeger, J. A., Field, D., &amp; Hammond, S. M. (1999). Measuring memory span. <em>International Journal of Psychology</em>, <em>34</em>(5-6), 359–363. <a href="https://doi.org/10.1080/002075999399693" class="external-link">https://doi.org/10.1080/002075999399693</a>
</div>
<div id="ref-gunelBayesFactorsIndependence1974" class="csl-entry" role="listitem">
Gûnel, E., &amp; Dickey, J. (1974). Bayes factors for independence in contingency tables. <em>Biometrika</em>, <em>61</em>(3), 545–557. <a href="https://doi.org/10.1093/biomet/61.3.545" class="external-link">https://doi.org/10.1093/biomet/61.3.545</a>
</div>
<div id="ref-haciomerogluObjectSpatialVisualizationVerbal2016" class="csl-entry" role="listitem">
Haciomeroglu, E. S. (2016). Object-spatial visualization and verbal cognitive styles, and their relation to cognitive abilities and mathematical performance. <em>Educational Sciences: Theory and Practice</em>, <em>16</em>(3), 987–1003. <a href="https://eric.ed.gov/?id=EJ1115147" class="external-link">https://eric.ed.gov/?id=EJ1115147</a>
</div>
<div id="ref-heatonWisconsinCardSorting1993" class="csl-entry" role="listitem">
Heaton, R. K., &amp; Staff, P. A. R. (1993). Wisconsin card sorting test: Computer version 2. <em>Odessa: Psychological Assessment Resources</em>, <em>4</em>, 1–4.
</div>
<div id="ref-hofflerMoreEvidenceThree2017" class="csl-entry" role="listitem">
Höffler, T. N., Koć-Januchta, M., &amp; Leutner, D. (2017). More evidence for three types of cognitive style: Validating the object-spatial imagery and verbal questionnaire using eye tracking when learning with texts and pictures. <em>Applied Cognitive Psychology</em>, <em>31</em>(1), 109–115. <a href="https://doi.org/10.1002/acp.3300" class="external-link">https://doi.org/10.1002/acp.3300</a>
</div>
<div id="ref-johnson-lairdMentalModelsHuman2010" class="csl-entry" role="listitem">
Johnson-Laird, P. N. (2010). Mental models and human reasoning. <em>Proceedings of the National Academy of Sciences</em>, <em>107</em>(43), 18243–18250. <a href="https://doi.org/10.1073/pnas.1012933107" class="external-link">https://doi.org/10.1073/pnas.1012933107</a>
</div>
<div id="ref-kassBayesFactors1995" class="csl-entry" role="listitem">
Kass, R. E., &amp; Raftery, A. E. (1995). Bayes factors. <em>Journal of the American Statistical Association</em>, <em>90</em>(430), 773–795. <a href="https://doi.org/10.1080/01621459.1995.10476572" class="external-link">https://doi.org/10.1080/01621459.1995.10476572</a>
</div>
<div id="ref-R-factoextra" class="csl-entry" role="listitem">
Kassambara, A., &amp; Mundt, F. (2020). <em>Factoextra: Extract and visualize the results of multivariate data analyses</em>. <a href="http://www.sthda.com/english/rpkgs/factoextra" class="external-link">http://www.sthda.com/english/rpkgs/factoextra</a>
</div>
<div id="ref-keoghBlindMindNo2018" class="csl-entry" role="listitem">
Keogh, R., &amp; Pearson, J. (2018). The blind mind: No sensory visual imagery in aphantasia. <em>Cortex</em>, <em>105</em>, 53–60. <a href="https://doi.org/10.1016/j.cortex.2017.10.012" class="external-link">https://doi.org/10.1016/j.cortex.2017.10.012</a>
</div>
<div id="ref-keoghVisualWorkingMemory2021" class="csl-entry" role="listitem">
Keogh, R., Wicken, M., &amp; Pearson, J. (2021). Visual working memory in aphantasia: Retained accuracy and capacity with a different strategy. <em>Cortex</em>, <em>143</em>, 237–253. <a href="https://doi.org/10.1016/j.cortex.2021.07.012" class="external-link">https://doi.org/10.1016/j.cortex.2021.07.012</a>
</div>
<div id="ref-keoghFewerIntrusiveMemories2023" class="csl-entry" role="listitem">
Keogh, R., Wicken, M., &amp; Pearson, J. (2023). <em>Fewer intrusive memories in aphantasia: Using the trauma film paradigm as a laboratory model of PTSD</em>.
</div>
<div id="ref-knauffVisualImageryCan2002" class="csl-entry" role="listitem">
Knauff, M., &amp; Johnson-Laird, P. N. (2002). Visual imagery can impede reasoning. <em>Memory &amp; Cognition</em>, <em>30</em>(3), 363–371. <a href="https://doi.org/10.3758/BF03194937" class="external-link">https://doi.org/10.3758/BF03194937</a>
</div>
<div id="ref-knightMemoryImageryNo2022" class="csl-entry" role="listitem">
Knight, K. F., Milton, F. N., &amp; Zeman, A. (2022). Memory without imagery: No evidence of visual working memory impairment in people with aphantasia. <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em>, <em>44</em>, 8.
</div>
<div id="ref-kongsWCST64WisconsinCard2000" class="csl-entry" role="listitem">
Kongs, S. K., Thompson, L. L., Iverson, G. L., &amp; Heaton, R. K. (2000). <em>WCST-64: Wisconsin card sorting test- 64 card version, professional manual</em>. PAR.
</div>
<div id="ref-koppReliabilityWisconsinCard2021" class="csl-entry" role="listitem">
Kopp, B., Lange, F., &amp; Steinke, A. (2021). The reliability of the wisconsin card sorting test in clinical practice. <em>Assessment</em>, <em>28</em>(1), 248–263. <a href="https://doi.org/10.1177/1073191119866257" class="external-link">https://doi.org/10.1177/1073191119866257</a>
</div>
<div id="ref-kosslynCognitiveNeuroscienceMental1995" class="csl-entry" role="listitem">
Kosslyn, S. M., Behrmann, M., &amp; Jeannerod, M. (1995). The cognitive neuroscience of mental imagery. <em>Neuropsychologia</em>, <em>33</em>(11), 1335–1344. <a href="https://doi.org/10.1016/0028-3932(95)00067-D" class="external-link">https://doi.org/10.1016/0028-3932(95)00067-D</a>
</div>
<div id="ref-kozhevnikovTradeoffObjectSpatial2010" class="csl-entry" role="listitem">
Kozhevnikov, M., Blazhenkova, O., &amp; Becker, M. (2010). Trade-off in object versus spatial visualization abilities: Restriction in the development of visual-processing resources. <em>Psychonomic Bulletin &amp; Review</em>, <em>17</em>(1), 29–35. <a href="https://doi.org/10.3758/PBR.17.1.29" class="external-link">https://doi.org/10.3758/PBR.17.1.29</a>
</div>
<div id="ref-kozhevnikovDissociationObjectManipulation2001" class="csl-entry" role="listitem">
Kozhevnikov, M., &amp; Hegarty, M. (2001). A dissociation between object manipulation spatial ability and spatial orientation ability. <em>Memory &amp; Cognition</em>, <em>29</em>(5), 745–756. <a href="https://doi.org/10.3758/BF03200477" class="external-link">https://doi.org/10.3758/BF03200477</a>
</div>
<div id="ref-kozhevnikovRevisingVisualizerVerbalizerDimension2002" class="csl-entry" role="listitem">
Kozhevnikov, M., Hegarty, M., &amp; Mayer, R. E. (2002). Revising the visualizer-verbalizer dimension: Evidence for two types of visualizers. <em>Cognition and Instruction</em>, <em>20</em>(1), 47–77. <a href="http://www.jstor.org/stable/3233862" class="external-link">http://www.jstor.org/stable/3233862</a>
</div>
<div id="ref-kozhevnikovSpatialObjectVisualizers2005" class="csl-entry" role="listitem">
Kozhevnikov, M., Kosslyn, S., &amp; Shephard, J. (2005). Spatial versus object visualizers: A new characterization of visual cognitive style. <em>Memory &amp; Cognition</em>, <em>33</em>(4), 710–726. <a href="https://doi.org/10.3758/BF03195337" class="external-link">https://doi.org/10.3758/BF03195337</a>
</div>
<div id="ref-krempelAphantasiaInvoluntaryImagery2024" class="csl-entry" role="listitem">
Krempel, R., &amp; Monzel, M. (2024). Aphantasia and involuntary imagery. <em>Consciousness and Cognition</em>, <em>120</em>, 103679. <a href="https://doi.org/10.1016/j.concog.2024.103679" class="external-link">https://doi.org/10.1016/j.concog.2024.103679</a>
</div>
<div id="ref-kyriazosDealingMulticollinearityFactor2023" class="csl-entry" role="listitem">
Kyriazos, T., &amp; Poga, M. (2023). Dealing with multicollinearity in factor analysis: The problem, detections, and solutions. <em>Open Journal of Statistics</em>, <em>13</em>(33), 404–424. <a href="https://doi.org/10.4236/ojs.2023.133020" class="external-link">https://doi.org/10.4236/ojs.2023.133020</a>
</div>
<div id="ref-langeJustAnotherTool2015" class="csl-entry" role="listitem">
Lange, K., Kühn, S., &amp; Filevich, E. (2015). "Just another tool for online studies” (JATOS): An easy solution for setup and management of web servers supporting online studies. <em>PLOS ONE</em>, <em>10</em>(6), e0130834. <a href="https://doi.org/10.1371/journal.pone.0130834" class="external-link">https://doi.org/10.1371/journal.pone.0130834</a>
</div>
<div id="ref-leeuwJsPsychEnablingOpenSource2023" class="csl-entry" role="listitem">
Leeuw, J. R. de, Gilbert, R. A., &amp; Luchterhandt, B. (2023). jsPsych: Enabling an open-source collaborative ecosystem of behavioral experiments. <em>Journal of Open Source Software</em>, <em>8</em>(85), 5351. <a href="https://doi.org/10.21105/joss.05351" class="external-link">https://doi.org/10.21105/joss.05351</a>
</div>
<div id="ref-R-emmeans" class="csl-entry" role="listitem">
Lenth, R. V. (2024). <em>Emmeans: Estimated marginal means, aka least-squares means</em>. <a href="https://rvlenth.github.io/emmeans/" class="external-link">https://rvlenth.github.io/emmeans/</a>
</div>
<div id="ref-see2021" class="csl-entry" role="listitem">
Lüdecke, D., Patil, I., Ben-Shachar, M. S., Wiernik, B. M., Waggoner, P., &amp; Makowski, D. (2021). <span class="nocase">see</span>: An <span>R</span> package for visualizing statistical models. <em>Journal of Open Source Software</em>, <em>6</em>(64), 3393. <a href="https://doi.org/10.21105/joss.03393" class="external-link">https://doi.org/10.21105/joss.03393</a>
</div>
<div id="ref-bayestestR2019" class="csl-entry" role="listitem">
Makowski, D., Ben-Shachar, M. S., &amp; Lüdecke, D. (2019). bayestestR: Describing effects and their uncertainty, existence and significance within the bayesian framework. <em>Journal of Open Source Software</em>, <em>4</em>(40), 1541. <a href="https://doi.org/10.21105/joss.01541" class="external-link">https://doi.org/10.21105/joss.01541</a>
</div>
<div id="ref-marksVisualImageryDifferences1973" class="csl-entry" role="listitem">
Marks, D. F. (1973). Visual imagery differences in the recall of pictures. <em>British Journal of Psychology</em>, <em>64</em>(1), 17–24. <a href="https://doi.org/10.1111/j.2044-8295.1973.tb01322.x" class="external-link">https://doi.org/10.1111/j.2044-8295.1973.tb01322.x</a>
</div>
<div id="ref-mathotOpenSesameOpensourceGraphical2012" class="csl-entry" role="listitem">
Mathôt, S., Schreij, D., &amp; Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. <em>Behavior Research Methods</em>, <em>44</em>(2), 314–324. <a href="https://doi.org/10.3758/s13428-011-0168-7" class="external-link">https://doi.org/10.3758/s13428-011-0168-7</a>
</div>
<div id="ref-mckelvieVVIQPsychometricTest1995" class="csl-entry" role="listitem">
McKelvie, S. J. (1995). The VVIQ as a psychometric test of individual differences in visual imagery vividness: A critical quantitative review and plea for direction. <em>Journal of Mental Imagery</em>, <em>19</em>(3-4), 1–106.
</div>
<div id="ref-miltonBehavioralNeuralSignatures2021" class="csl-entry" role="listitem">
Milton, F., Fulford, J., Dance, C., Gaddum, J., Heuerman-Williamson, B., Jones, K., Knight, K. F., MacKisack, M., Winlove, C., &amp; Zeman, A. (2021). Behavioral and neural signatures of visual imagery vividness extremes: Aphantasia versus hyperphantasia. <em>Cerebral Cortex Communications</em>, <em>2</em>(2), tgab035. <a href="https://doi.org/10.1093/texcom/tgab035" class="external-link">https://doi.org/10.1093/texcom/tgab035</a>
</div>
<div id="ref-monzelAphantasiaFrameworkNeurodivergence2023" class="csl-entry" role="listitem">
Monzel, M., Dance, C., Azañón, E., &amp; Simner, J. (2023). Aphantasia within the framework of neurodivergence: Some preliminary data and the curse of the confidence gap. <em>Consciousness and Cognition</em>, <em>115</em>, 103567. <a href="https://doi.org/10.1016/j.concog.2023.103567" class="external-link">https://doi.org/10.1016/j.concog.2023.103567</a>
</div>
<div id="ref-monzelHippocampaloccipitalConnectivityReflects2023" class="csl-entry" role="listitem">
Monzel, M., Leelaarporn, P., Lutz, T., Schultz, J., Brunheim, S., Reuter, M., &amp; McCormick, C. (2023). <em>Hippocampal-occipital connectivity reflects autobiographical memory deficits in aphantasia</em>. <a href="https://doi.org/10.1101/2023.08.11.552915" class="external-link">https://doi.org/10.1101/2023.08.11.552915</a>
</div>
<div id="ref-R-BayesFactor" class="csl-entry" role="listitem">
Morey, R. D., &amp; Rouder, J. N. (2024). <em>BayesFactor: Computation of bayes factors for common designs</em>. <a href="https://richarddmorey.github.io/BayesFactor/" class="external-link">https://richarddmorey.github.io/BayesFactor/</a>
</div>
<div id="ref-murakiInsightsEmbodiedCognition2023" class="csl-entry" role="listitem">
Muraki, E. J., Speed, L. J., &amp; Pexman, P. M. (2023). Insights into embodied cognition and mental imagery from aphantasia. <em>Nature Reviews Psychology</em>. <a href="https://doi.org/10.1038/s44159-023-00221-9" class="external-link">https://doi.org/10.1038/s44159-023-00221-9</a>
</div>
<div id="ref-nelsonModifiedCardSorting1976" class="csl-entry" role="listitem">
Nelson, H. E. (1976). A modified card sorting test sensitive to frontal lobe defects. <em>Cortex</em>, <em>12</em>(4), 313–324. <a href="https://doi.org/10.1016/S0010-9452(76)80035-4" class="external-link">https://doi.org/10.1016/S0010-9452(76)80035-4</a>
</div>
<div id="ref-paivioImageryAbilityVisual1971" class="csl-entry" role="listitem">
Paivio, A., &amp; Ernest, C. H. (1971). Imagery ability and visual perception of verbal and nonverbal stimuli. <em>Perception &amp; Psychophysics</em>, <em>10</em>(6), 429–432. <a href="https://doi.org/10.3758/BF03210327" class="external-link">https://doi.org/10.3758/BF03210327</a>
</div>
<div id="ref-palermoCongenitalLackExtraordinary2022" class="csl-entry" role="listitem">
Palermo, L., Boccia, M., Piccardi, L., &amp; Nori, R. (2022). Congenital lack and extraordinary ability in object and spatial imagery: An investigation on sub-types of aphantasia and hyperphantasia. <em>Consciousness and Cognition</em>, <em>103</em>, 103360. <a href="https://doi.org/10.1016/j.concog.2022.103360" class="external-link">https://doi.org/10.1016/j.concog.2022.103360</a>
</div>
<div id="ref-pearsonHumanImaginationCognitive2019" class="csl-entry" role="listitem">
Pearson, J. (2019). The human imagination: The cognitive neuroscience of visual mental imagery. <em>Nature Reviews Neuroscience</em>, <em>20</em>(1010), 624–634. <a href="https://doi.org/10.1038/s41583-019-0202-9" class="external-link">https://doi.org/10.1038/s41583-019-0202-9</a>
</div>
<div id="ref-pearsonRedefiningVisualWorking2019" class="csl-entry" role="listitem">
Pearson, J., &amp; Keogh, R. (2019). Redefining visual working memory: A cognitive-strategy, brain-region approach. <em>Current Directions in Psychological Science</em>, <em>28</em>(3), 266–273. <a href="https://doi.org/10.1177/0963721419835210" class="external-link">https://doi.org/10.1177/0963721419835210</a>
</div>
<div id="ref-R-patchwork" class="csl-entry" role="listitem">
Pedersen, T. L. (2024). <em>Patchwork: The composer of plots</em>. <a href="https://patchwork.data-imaginist.com" class="external-link">https://patchwork.data-imaginist.com</a>
</div>
<div id="ref-Rstudio" class="csl-entry" role="listitem">
Posit team. (2025). <em>RStudio: Integrated development environment for r</em>. Posit Software, PBC. <a href="http://www.posit.co/" class="external-link">http://www.posit.co/</a>
</div>
<div id="ref-pounderOnlyMinimalDifferences2022" class="csl-entry" role="listitem">
Pounder, Z., Jacob, J., Evans, S., Loveday, C., Eardley, A. F., &amp; Silvanto, J. (2022). Only minimal differences between individuals with congenital aphantasia and those with typical imagery on neuropsychological tasks that involve imagery. <em>Cortex</em>, <em>148</em>, 180–192. <a href="https://doi.org/10.1016/j.cortex.2021.12.010" class="external-link">https://doi.org/10.1016/j.cortex.2021.12.010</a>
</div>
<div id="ref-psutkaSampleSizeMaximumlikelihood2019" class="csl-entry" role="listitem">
Psutka, J. V., &amp; Psutka, J. (2019). Sample size for maximum-likelihood estimates of gaussian model depending on dimensionality of pattern space. <em>Pattern Recognition</em>, <em>91</em>, 25–33. <a href="https://doi.org/10.1016/j.patcog.2019.01.046" class="external-link">https://doi.org/10.1016/j.patcog.2019.01.046</a>
</div>
<div id="ref-purkartAreThereUnconscious2024" class="csl-entry" role="listitem">
Purkart, R., Delem, M., Ranson, V., Andrey, C., Versace, R., Cavalli, E., &amp; Plancher, G. (2024). Are there unconscious visual images in aphantasia? Development of an implicit priming paradigm. <em>Cognition</em>, <em>256</em>, 106059. <a href="https://doi.org/10.1016/j.cognition.2024.106059" class="external-link">https://doi.org/10.1016/j.cognition.2024.106059</a>
</div>
<div id="ref-R-base" class="csl-entry" role="listitem">
R Core Team. (2024). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="external-link">https://www.R-project.org/</a>
</div>
<div id="ref-ramfulMeasurementSpatialAbility2017" class="csl-entry" role="listitem">
Ramful, A., Lowrie, T., &amp; Logan, T. (2017). Measurement of spatial ability: Construction and validation of the spatial reasoning instrument for middle school students. <em>Journal of Psychoeducational Assessment</em>, <em>35</em>(7), 709–727. <a href="https://doi.org/10.1177/0734282916659207" class="external-link">https://doi.org/10.1177/0734282916659207</a>
</div>
<div id="ref-ravenRavenProgressiveMatrices1938" class="csl-entry" role="listitem">
Raven, J. C., &amp; Court, J. H. (1938). <em>Raven’s progressive matrices</em>. Western Psychological Services Los Angeles.
</div>
<div id="ref-reederNonvisualSpatialStrategies2024" class="csl-entry" role="listitem">
Reeder, R. R., Pounder, Z., Figueroa, A., Jüllig, A., &amp; Azañón, E. (2024). Non-visual spatial strategies are effective for maintaining precise information in visual working memory. <em>Cognition</em>, <em>251</em>, 105907. <a href="https://doi.org/10.1016/j.cognition.2024.105907" class="external-link">https://doi.org/10.1016/j.cognition.2024.105907</a>
</div>
<div id="ref-richardsonMeaningMeasurementMemory1977" class="csl-entry" role="listitem">
Richardson, A. (1977). The meaning and measurement of memory imagery. <em>British Journal of Psychology</em>, <em>68</em>(1), 29–43. <a href="https://doi.org/10.1111/j.2044-8295.1977.tb01556.x" class="external-link">https://doi.org/10.1111/j.2044-8295.1977.tb01556.x</a>
</div>
<div id="ref-mclust2023" class="csl-entry" role="listitem">
Scrucca, L., Fraley, C., Murphy, T. B., &amp; Raftery, A. E. (2023). <em>Model-based clustering, classification, and density estimation using <span class="nocase">mclust</span> in <span>R</span></em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781003277965" class="external-link">https://doi.org/10.1201/9781003277965</a>
</div>
<div id="ref-shepardMentalRotationThreeDimensional1971" class="csl-entry" role="listitem">
Shepard, R. N., &amp; Metzler, J. (1971). Mental rotation of three-dimensional objects. <em>Science</em>, <em>171</em>(3972), 701–703. <a href="https://doi.org/10.1126/science.171.3972.701" class="external-link">https://doi.org/10.1126/science.171.3972.701</a>
</div>
<div id="ref-steinleyEvaluatingMixtureModeling2011" class="csl-entry" role="listitem">
Steinley, D., &amp; Brusco, M. J. (2011). Evaluating mixture modeling for clustering: Recommendations and cautions. <em>Psychological Methods</em>, <em>16</em>(1), 63–79. <a href="https://doi.org/10.1037/a0022673" class="external-link">https://doi.org/10.1037/a0022673</a>
</div>
<div id="ref-suggateMentalImagerySkill2022" class="csl-entry" role="listitem">
Suggate, S., &amp; Lenhard, W. (2022). Mental imagery skill predicts adults’ reading performance. <em>Learning and Instruction</em>, <em>80</em>, 101633. <a href="https://doi.org/10.1016/j.learninstruc.2022.101633" class="external-link">https://doi.org/10.1016/j.learninstruc.2022.101633</a>
</div>
<div id="ref-vannucciIndividualDifferencesVisuospatial2006" class="csl-entry" role="listitem">
Vannucci, M., Cioli, L., Chiorri, C., Grazi, A., &amp; Kozhevnikov, M. (2006). Individual differences in visuo-spatial imagery: Further evidence for the distinction between object and spatial imagers. <em>Cognitive Processing</em>, <em>7</em>(1), 144–145. <a href="https://doi.org/10.1007/s10339-006-0108-0" class="external-link">https://doi.org/10.1007/s10339-006-0108-0</a>
</div>
<div id="ref-waiSpatialAbilitySTEM2009" class="csl-entry" role="listitem">
Wai, J., Lubinski, D., &amp; Benbow, C. P. (2009). Spatial ability for STEM domains: Aligning over 50 years of cumulative psychological knowledge solidifies its importance. <em>Journal of Educational Psychology</em>, <em>101</em>, 817–835. <a href="https://doi.org/10.1037/a0016127" class="external-link">https://doi.org/10.1037/a0016127</a>
</div>
<div id="ref-wallaceImageryVividnessHypnotic1990" class="csl-entry" role="listitem">
Wallace, B. (1990). Imagery vividness, hypnotic susceptibility, and the perception of fragmented stimuli. <em>Journal of Personality and Social Psychology</em>, <em>58</em>(2), 354–359. <a href="https://doi.org/10.1037/0022-3514.58.2.354" class="external-link">https://doi.org/10.1037/0022-3514.58.2.354</a>
</div>
<div id="ref-wechslerWechslerAdultIntelligence2008" class="csl-entry" role="listitem">
Wechsler, D., Coalson, D. L., &amp; Raiford, S. E. (2008). <em>Wechsler adult intelligence scale (technical and interpretive manual 4th ed.). San antonio: NCS pearson</em>. Inc.
</div>
<div id="ref-wickenCriticalRoleMental2021" class="csl-entry" role="listitem">
Wicken, M., Keogh, R., &amp; Pearson, J. (2021). The critical role of mental imagery in human emotion: Insights from fear-based imagery and aphantasia. <em>Proceedings of the Royal Society B: Biological Sciences</em>, <em>288</em>(1946), 20210267. <a href="https://doi.org/10.1098/rspb.2021.0267" class="external-link">https://doi.org/10.1098/rspb.2021.0267</a>
</div>
<div id="ref-ggplot22016" class="csl-entry" role="listitem">
Wickham, H. (2016). <em>ggplot2: Elegant graphics for data analysis</em>. Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org" class="external-link">https://ggplot2.tidyverse.org</a>
</div>
<div id="ref-tidyverse2019" class="csl-entry" role="listitem">
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the <span class="nocase">tidyverse</span>. <em>Journal of Open Source Software</em>, <em>4</em>(43), 1686. <a href="https://doi.org/10.21105/joss.01686" class="external-link">https://doi.org/10.21105/joss.01686</a>
</div>
<div id="ref-zakharovApplicationKmeansClustering2016" class="csl-entry" role="listitem">
Zakharov, K. (2016). Application of k-means clustering in psychological studies. <em>The Quantitative Methods for Psychology</em>, <em>12</em>(2), 87–100. <a href="https://doi.org/10.20982/tqmp.12.2.p087" class="external-link">https://doi.org/10.20982/tqmp.12.2.p087</a>
</div>
<div id="ref-zemanLivesImageryCongenital2015" class="csl-entry" role="listitem">
Zeman, A., Dewar, M., &amp; Della Sala, S. (2015). Lives without imagery – congenital aphantasia. <em>Cortex</em>, <em>73</em>, 378–380. <a href="https://doi.org/10.1016/j.cortex.2015.05.019" class="external-link">https://doi.org/10.1016/j.cortex.2015.05.019</a>
</div>
<div id="ref-zemanPhantasiaPsychologicalSignificance2020" class="csl-entry" role="listitem">
Zeman, A., Milton, F., Della Sala, S., Dewar, M., Frayling, T., Gaddum, J., Hattersley, A., Heuerman-Williamson, B., Jones, K., MacKisack, M., &amp; al., et. (2020). Phantasia–the psychological significance of lifelong visual imagery vividness extremes. <em>Cortex</em>, <em>130</em>, 426–440. <a href="https://doi.org/10.1016/j.cortex.2020.04.003" class="external-link">https://doi.org/10.1016/j.cortex.2020.04.003</a>
</div>
</div>
</section><section class="section level2"><h2 class="unnumbered" id="tables">Tables<a class="anchor" aria-label="anchor" href="#tables"></a>
</h2>
<div class="cell">
<div id="tbl-g-results" class="cell quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-g-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 1: Means and standard deviations (SD) of the scores of each VVIQ group for every variable. The score differences, their 95% Credible Intervals (CrI) and weights of evidence are reported for each variable.
</figcaption><div aria-describedby="tbl-g-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top">
<colgroup>
<col style="width: 18%">
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 19%">
<col style="width: 14%">
<col style="width: 13%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Variable</th>
<th style="text-align: left;">Control (n = 51)</th>
<th style="text-align: left;">Aphantasia (n = 45)</th>
<th style="text-align: right;">Difference (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>)</th>
<th style="text-align: left;">95% CrI</th>
<th style="text-align: right;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">VVIQ</td>
<td style="text-align: left;">58.08 (10.81)</td>
<td style="text-align: left;">18.33 (4.24)</td>
<td style="text-align: right;">39.67</td>
<td style="text-align: left;">[36.21, 43.18]</td>
<td style="text-align: right;">64.33</td>
</tr>
<tr class="even">
<td style="text-align: left;">OSIVQ-Object</td>
<td style="text-align: left;">54.04 (9.63)</td>
<td style="text-align: left;">24.6 (6.96)</td>
<td style="text-align: right;">29.37</td>
<td style="text-align: left;">[25.76, 32.76]</td>
<td style="text-align: right;">37.94</td>
</tr>
<tr class="odd">
<td style="text-align: left;">OSIVQ-Spatial</td>
<td style="text-align: left;">43.59 (9.25)</td>
<td style="text-align: left;">39.22 (8.75)</td>
<td style="text-align: right;">4.46</td>
<td style="text-align: left;">[0.84, 8.29]</td>
<td style="text-align: right;">-0.70</td>
</tr>
<tr class="even">
<td style="text-align: left;">OSIVQ-Verbal</td>
<td style="text-align: left;">43.22 (7.52)</td>
<td style="text-align: left;">49.38 (11.13)</td>
<td style="text-align: right;">-6.39</td>
<td style="text-align: left;">[-10.12, -2.53]</td>
<td style="text-align: right;">1.63</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Psi-Q Vision</td>
<td style="text-align: left;">8.33 (1.47)</td>
<td style="text-align: left;">1.76 (1.21)</td>
<td style="text-align: right;">6.55</td>
<td style="text-align: left;">[6, 7.1]</td>
<td style="text-align: right;">58.94</td>
</tr>
<tr class="even">
<td style="text-align: left;">Psi-Q Audition</td>
<td style="text-align: left;">8.08 (1.74)</td>
<td style="text-align: left;">2.92 (2.79)</td>
<td style="text-align: right;">5.08</td>
<td style="text-align: left;">[4.15, 5.96]</td>
<td style="text-align: right;">24.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Psi-Q Smell</td>
<td style="text-align: left;">7.01 (2.36)</td>
<td style="text-align: left;">1.78 (1.61)</td>
<td style="text-align: right;">5.21</td>
<td style="text-align: left;">[4.38, 6.05]</td>
<td style="text-align: right;">26.51</td>
</tr>
<tr class="even">
<td style="text-align: left;">Psi-Q Taste</td>
<td style="text-align: left;">6.88 (2.37)</td>
<td style="text-align: left;">2.14 (2.15)</td>
<td style="text-align: right;">4.66</td>
<td style="text-align: left;">[3.76, 5.59]</td>
<td style="text-align: right;">17.89</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Psi-Q Touch</td>
<td style="text-align: left;">8.36 (1.84)</td>
<td style="text-align: left;">2.64 (2.51)</td>
<td style="text-align: right;">5.62</td>
<td style="text-align: left;">[4.73, 6.47]</td>
<td style="text-align: right;">30.68</td>
</tr>
<tr class="even">
<td style="text-align: left;">Psi-Q Sensations</td>
<td style="text-align: left;">7.93 (1.9)</td>
<td style="text-align: left;">2.99 (2.67)</td>
<td style="text-align: right;">4.82</td>
<td style="text-align: left;">[3.93, 5.73]</td>
<td style="text-align: right;">19.56</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Psi-Q Feelings</td>
<td style="text-align: left;">7.8 (2.11)</td>
<td style="text-align: left;">3.5 (2.97)</td>
<td style="text-align: right;">4.16</td>
<td style="text-align: left;">[3.17, 5.14]</td>
<td style="text-align: right;">14.29</td>
</tr>
<tr class="even">
<td style="text-align: left;">Raven matrices</td>
<td style="text-align: left;">12.67 (3.23)</td>
<td style="text-align: left;">13.93 (2.27)</td>
<td style="text-align: right;">-1.22</td>
<td style="text-align: left;">[-2.35, -0.03]</td>
<td style="text-align: right;">-1.30</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SRI</td>
<td style="text-align: left;">21.18 (4.86)</td>
<td style="text-align: left;">22.58 (3.8)</td>
<td style="text-align: right;">-1.45</td>
<td style="text-align: left;">[-3.3, 0.28]</td>
<td style="text-align: right;">-2.20</td>
</tr>
<tr class="even">
<td style="text-align: left;">Digit span</td>
<td style="text-align: left;">4.07 (0.98)</td>
<td style="text-align: left;">4.2 (1.19)</td>
<td style="text-align: right;">-0.14</td>
<td style="text-align: left;">[-0.59, 0.31]</td>
<td style="text-align: right;">-3.28</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Spatial span</td>
<td style="text-align: left;">3.1 (1.25)</td>
<td style="text-align: left;">3.53 (0.76)</td>
<td style="text-align: right;">-0.47</td>
<td style="text-align: left;">[-0.91, -0.07]</td>
<td style="text-align: right;">-1.01</td>
</tr>
<tr class="even">
<td style="text-align: left;">Similarities test</td>
<td style="text-align: left;">23 (5.22)</td>
<td style="text-align: left;">24.31 (4.38)</td>
<td style="text-align: right;">-1.25</td>
<td style="text-align: left;">[-3.18, 0.71]</td>
<td style="text-align: right;">-2.68</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WCST</td>
<td style="text-align: left;">62.8 (15.46)</td>
<td style="text-align: left;">64.93 (13.63)</td>
<td style="text-align: right;">-2.18</td>
<td style="text-align: left;">[-8.04, 3.73]</td>
<td style="text-align: right;">-3.22</td>
</tr>
<tr class="even">
<td style="text-align: left;">Reading</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">comprehension</td>
<td style="text-align: left;">16.9 (6.8)</td>
<td style="text-align: left;">19.11 (5.15)</td>
<td style="text-align: right;">-2.20</td>
<td style="text-align: left;">[-4.67, 0.26]</td>
<td style="text-align: right;">-1.94</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<div id="tbl-c-contrasts" class="cell quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-c-contrasts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 2: Pairwise comparisons between the three clusters for the seven clustering variables and the three external variables (Auditory imagery, WCST and Reading comprehension). The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math> quantifies the weight of evidence in favour of a difference between clusters.
</figcaption><div aria-describedby="tbl-c-contrasts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top">
<colgroup>
<col style="width: 21%">
<col style="width: 26%">
<col style="width: 22%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Variable</th>
<th style="text-align: left;">Comparison</th>
<th style="text-align: right;">Difference (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>)</th>
<th style="text-align: left;">95% CrI</th>
<th style="text-align: right;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><msub><mi>F</mi><mn>10</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">log(BF_{10})</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Visual imagery</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.28</td>
<td style="text-align: left;">[-0.36, -0.2]</td>
<td style="text-align: right;">10.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">Visual imagery</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">-0.63</td>
<td style="text-align: left;">[-0.71, -0.55]</td>
<td style="text-align: right;">29.07</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Visual imagery</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">-0.35</td>
<td style="text-align: left;">[-0.43, -0.27]</td>
<td style="text-align: right;">14.36</td>
</tr>
<tr class="even">
<td style="text-align: left;">Auditory imagery</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.46</td>
<td style="text-align: left;">[-0.59, -0.33]</td>
<td style="text-align: right;">11.37</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Auditory imagery</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">-0.70</td>
<td style="text-align: left;">[-0.82, -0.57]</td>
<td style="text-align: right;">21.61</td>
</tr>
<tr class="even">
<td style="text-align: left;">Auditory imagery</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">-0.24</td>
<td style="text-align: left;">[-0.37, -0.12]</td>
<td style="text-align: right;">2.33</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sensory imagery</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.42</td>
<td style="text-align: left;">[-0.53, -0.31]</td>
<td style="text-align: right;">13.59</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sensory imagery</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">-0.67</td>
<td style="text-align: left;">[-0.77, -0.56]</td>
<td style="text-align: right;">27.02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sensory imagery</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">-0.25</td>
<td style="text-align: left;">[-0.36, -0.14]</td>
<td style="text-align: right;">3.99</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spatial imagery</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.13</td>
<td style="text-align: left;">[-0.19, -0.08]</td>
<td style="text-align: right;">5.52</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Spatial imagery</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: left;">[0, 0.11]</td>
<td style="text-align: right;">-1.74</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spatial imagery</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: left;">[0.13, 0.24]</td>
<td style="text-align: right;">13.55</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Verbal strategies</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">0.19</td>
<td style="text-align: left;">[0.12, 0.27]</td>
<td style="text-align: right;">6.86</td>
</tr>
<tr class="even">
<td style="text-align: left;">Verbal strategies</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: left;">[0.08, 0.23]</td>
<td style="text-align: right;">4.31</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Verbal strategies</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">-0.04</td>
<td style="text-align: left;">[-0.11, 0.03]</td>
<td style="text-align: right;">-3.19</td>
</tr>
<tr class="even">
<td style="text-align: left;">Raven +</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Digit Span</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.03</td>
<td style="text-align: left;">[-0.09, 0.02]</td>
<td style="text-align: right;">-2.38</td>
</tr>
<tr class="even">
<td style="text-align: left;">Raven +</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Digit Span</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: left;">[-0.04, 0.06]</td>
<td style="text-align: right;">-3.19</td>
</tr>
<tr class="even">
<td style="text-align: left;">Raven +</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Digit Span</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: left;">[-0.01, 0.09]</td>
<td style="text-align: right;">-2.20</td>
</tr>
<tr class="even">
<td style="text-align: left;">Verbal reasoning</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.08</td>
<td style="text-align: left;">[-0.15, -0.02]</td>
<td style="text-align: right;">-0.29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Verbal reasoning</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: left;">[-0.02, 0.11]</td>
<td style="text-align: right;">-2.26</td>
</tr>
<tr class="even">
<td style="text-align: left;">Verbal reasoning</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: left;">[0.06, 0.19]</td>
<td style="text-align: right;">3.33</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Spatial span std.</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.07</td>
<td style="text-align: left;">[-0.16, 0.01]</td>
<td style="text-align: right;">-1.91</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spatial span std.</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.11</td>
<td style="text-align: left;">[0.03, 0.2]</td>
<td style="text-align: right;">0.16</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Spatial span std.</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: left;">[0.1, 0.27]</td>
<td style="text-align: right;">4.23</td>
</tr>
<tr class="even">
<td style="text-align: left;">WCST</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.02</td>
<td style="text-align: left;">[-0.09, 0.05]</td>
<td style="text-align: right;">-3.15</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WCST</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: left;">[-0.01, 0.13]</td>
<td style="text-align: right;">-1.87</td>
</tr>
<tr class="even">
<td style="text-align: left;">WCST</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: left;">[0.01, 0.15]</td>
<td style="text-align: right;">-1.26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Reading</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">comprehension</td>
<td style="text-align: left;">A (Aphant.) - B (Mixed)</td>
<td style="text-align: right;">-0.03</td>
<td style="text-align: left;">[-0.13, 0.07]</td>
<td style="text-align: right;">-3.11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Reading</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">comprehension</td>
<td style="text-align: left;">A (Aphant.) - C (Control)</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: left;">[-0.03, 0.17]</td>
<td style="text-align: right;">-2.24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Reading</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">comprehension</td>
<td style="text-align: left;">B (Mixed) - C (Control)</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: left;">[0, 0.2]</td>
<td style="text-align: right;">-1.55</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</section><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config);
    }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      }
    });

    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });

  };

})();
          </script></main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Maël Delem.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
